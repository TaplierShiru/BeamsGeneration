{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4ba5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "\n",
    "# example of loading the generator model and generating images\n",
    "from numpy import asarray\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b2a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8c0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (64, 64, 3)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Size of the noise vector\n",
    "noise_dim = 128\n",
    "\n",
    "train_images_path = []\n",
    "train_images_path += glob.glob('../data/faces_50k/*.jpg')\n",
    "train_images_path += glob.glob('../data/second_dataset_69k/images/*.jpg')\n",
    "#train_images_path += glob.glob('../data/third_data_21k/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee083a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "  images = (images - 127.5) / 127.5\n",
    "  return images.astype('float32')\n",
    "\n",
    "def generator_img(path_list: list):\n",
    "    counter = 0\n",
    "    max_counter = len(path_list)\n",
    "    while True:\n",
    "        single_path = path_list[counter]\n",
    "        image_s = preprocess_images(\n",
    "            transform.resize(np.asarray(io.imread(single_path), dtype=np.float32),\n",
    "            (IMG_SHAPE[0], IMG_SHAPE[1]),\n",
    "            anti_aliasing=True                 \n",
    "        )).astype(np.float32)\n",
    "        yield image_s\n",
    "        # yield np.ones((336, 336, 3))\n",
    "        counter += 1\n",
    "\n",
    "        if counter == max_counter:\n",
    "            counter = 0\n",
    "            path_list = shuffle(path_list)\n",
    "\n",
    "def train_gen():\n",
    "    return generator_img(train_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da387b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_generator(\n",
    "        train_gen, \n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=IMG_SHAPE, dtype=np.float32)\n",
    "        )\n",
    "    )\n",
    "    .shuffle(BATCH_SIZE * 10).batch(BATCH_SIZE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b50f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 114788\n"
     ]
    }
   ],
   "source": [
    "train_size = len(train_images_path)\n",
    "\n",
    "print(f'train: {train_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddbe0d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 4,314,753\n",
      "Trainable params: 4,314,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def conv_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    use_bn=False,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.5,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_discriminator_model():\n",
    "    img_input = layers.Input(shape=IMG_SHAPE)\n",
    "    x = conv_block(\n",
    "        img_input,\n",
    "        64,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        use_bias=True,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    ) # 32\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        128,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    ) # 16\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        256,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    ) # 8\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        512,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )  # 4\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n",
    "    return d_model\n",
    "\n",
    "\n",
    "d_model = get_discriminator_model()\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0196f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 64)        73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 3)         1728      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 1,059,776\n",
      "Trainable params: 1,050,944\n",
      "Non-trainable params: 8,832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def upsample_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    up_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    use_bn=False,\n",
    "    use_bias=True,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.3,\n",
    "):\n",
    "    x = layers.UpSampling2D(up_size)(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_generator_model():\n",
    "    noise = layers.Input(shape=(noise_dim,))\n",
    "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Reshape((4, 4, 256))(x)\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        128,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    ) # 8\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        128,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    ) # 16\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        64,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    ) # 32\n",
    "    x = upsample_block(\n",
    "        x, IMG_SHAPE[-1], layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=False\n",
    "    ) # 64\n",
    "\n",
    "    g_model = keras.models.Model(noise, x, name=\"generator\")\n",
    "    return g_model\n",
    "\n",
    "\n",
    "g_model = get_generator_model()\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca57d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b620600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor():\n",
    "    def __init__(self, model, num_img=100, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        n = int(np.sqrt(self.num_img))\n",
    "        random_latent_vectors = np.random.normal(size=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.predict(random_latent_vectors)\n",
    "        # scale from [-1,1] to [0,1]\n",
    "        generated_images = (generated_images + 1) / 2.0\n",
    "        self._generate_plot(generated_images, n, f'{epoch}')\n",
    "    \n",
    "    def _generate_plot(self, examples, n, prefix):\n",
    "        # plot images\n",
    "        fig = plt.figure(figsize=(12,12))\n",
    "        for i in range(n * n):\n",
    "            # define subplot\n",
    "            plt.subplot(n, n, 1 + i)\n",
    "            # turn off axis\n",
    "            plt.axis('off')\n",
    "            # plot raw pixel data\n",
    "            plt.imshow(examples[i])\n",
    "        #pyplot.show()\n",
    "        fig.savefig(f'{prefix}_image.png')\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6bb91d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "# Set the number of epochs for trainining.\n",
    "epochs = 20\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "cbk = GANMonitor(g_model, num_img=100, latent_dim=noise_dim)\n",
    "\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cdfe85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 1/896, d=6.602, g=0.232\n",
      ">1, 2/896, d=-7.032, g=-1.593\n",
      ">1, 3/896, d=-27.527, g=8.962\n",
      ">1, 4/896, d=-38.136, g=22.169\n",
      ">1, 5/896, d=-45.867, g=46.973\n",
      ">1, 6/896, d=-49.926, g=50.742\n",
      ">1, 7/896, d=-56.552, g=45.699\n",
      ">1, 8/896, d=-49.996, g=54.152\n",
      ">1, 9/896, d=-44.751, g=47.128\n",
      ">1, 10/896, d=-35.395, g=27.163\n",
      ">1, 11/896, d=-26.905, g=10.751\n",
      ">1, 12/896, d=-20.776, g=12.277\n",
      ">1, 13/896, d=-22.946, g=18.212\n",
      ">1, 14/896, d=-22.069, g=18.200\n",
      ">1, 15/896, d=-21.580, g=34.634\n",
      ">1, 16/896, d=-18.946, g=34.606\n",
      ">1, 17/896, d=-20.772, g=44.959\n",
      ">1, 18/896, d=-17.753, g=35.492\n",
      ">1, 19/896, d=-17.600, g=34.683\n",
      ">1, 20/896, d=-17.184, g=18.918\n",
      ">1, 21/896, d=-14.760, g=24.090\n",
      ">1, 22/896, d=-16.237, g=34.982\n",
      ">1, 23/896, d=-16.484, g=36.275\n",
      ">1, 24/896, d=-15.641, g=42.963\n",
      ">1, 25/896, d=-15.813, g=29.029\n",
      ">1, 26/896, d=-14.901, g=32.694\n",
      ">1, 27/896, d=-15.728, g=46.054\n",
      ">1, 28/896, d=-15.954, g=47.990\n",
      ">1, 29/896, d=-14.532, g=36.887\n",
      ">1, 30/896, d=-17.459, g=19.748\n",
      ">1, 31/896, d=-14.750, g=13.214\n",
      ">1, 32/896, d=-14.443, g=8.384\n",
      ">1, 33/896, d=-18.283, g=37.408\n",
      ">1, 34/896, d=-13.500, g=38.650\n",
      ">1, 35/896, d=-13.381, g=15.013\n",
      ">1, 36/896, d=-19.400, g=17.440\n",
      ">1, 37/896, d=-14.409, g=31.715\n",
      ">1, 38/896, d=-12.836, g=48.467\n",
      ">1, 39/896, d=-11.747, g=2.345\n",
      ">1, 40/896, d=-18.844, g=64.283\n",
      ">1, 41/896, d=-15.181, g=1.812\n",
      ">1, 42/896, d=-14.579, g=19.399\n",
      ">1, 43/896, d=-13.065, g=3.361\n",
      ">1, 44/896, d=-12.335, g=52.350\n",
      ">1, 45/896, d=-18.965, g=43.587\n",
      ">1, 46/896, d=-16.786, g=38.256\n",
      ">1, 47/896, d=-9.968, g=21.379\n",
      ">1, 48/896, d=-11.400, g=30.405\n",
      ">1, 49/896, d=-16.437, g=6.862\n",
      ">1, 50/896, d=-16.949, g=36.189\n",
      ">1, 51/896, d=-12.442, g=-0.377\n",
      ">1, 52/896, d=-12.584, g=6.287\n",
      ">1, 53/896, d=-12.809, g=22.964\n",
      ">1, 54/896, d=-14.932, g=35.379\n",
      ">1, 55/896, d=-14.321, g=21.653\n",
      ">1, 56/896, d=-13.375, g=21.013\n",
      ">1, 57/896, d=-13.316, g=13.144\n",
      ">1, 58/896, d=-15.042, g=4.295\n",
      ">1, 59/896, d=-13.990, g=23.059\n",
      ">1, 60/896, d=-13.296, g=-6.706\n",
      ">1, 61/896, d=-12.894, g=10.161\n",
      ">1, 62/896, d=-13.638, g=18.628\n",
      ">1, 63/896, d=-18.758, g=66.728\n",
      ">1, 64/896, d=-16.035, g=3.045\n",
      ">1, 65/896, d=-11.796, g=-5.019\n",
      ">1, 66/896, d=-12.036, g=-2.655\n",
      ">1, 67/896, d=-16.296, g=34.169\n",
      ">1, 68/896, d=-16.438, g=8.930\n",
      ">1, 69/896, d=-12.716, g=26.380\n",
      ">1, 70/896, d=-12.361, g=28.735\n",
      ">1, 71/896, d=-18.691, g=-1.063\n",
      ">1, 72/896, d=-18.757, g=-3.829\n",
      ">1, 73/896, d=-12.565, g=-1.145\n",
      ">1, 74/896, d=-10.319, g=19.549\n",
      ">1, 75/896, d=-15.660, g=10.099\n",
      ">1, 76/896, d=-15.635, g=7.174\n",
      ">1, 77/896, d=-12.799, g=-21.154\n",
      ">1, 78/896, d=-12.613, g=44.272\n",
      ">1, 79/896, d=-14.260, g=22.971\n",
      ">1, 80/896, d=-13.890, g=31.418\n",
      ">1, 81/896, d=-14.703, g=0.214\n",
      ">1, 82/896, d=-8.589, g=12.484\n",
      ">1, 83/896, d=-10.375, g=-17.586\n",
      ">1, 84/896, d=-11.847, g=-9.625\n",
      ">1, 85/896, d=-12.064, g=4.137\n",
      ">1, 86/896, d=-15.717, g=22.789\n",
      ">1, 87/896, d=-15.220, g=34.854\n",
      ">1, 88/896, d=-12.340, g=-7.845\n",
      ">1, 89/896, d=-11.759, g=-9.981\n",
      ">1, 90/896, d=-7.818, g=22.889\n",
      ">1, 91/896, d=-9.238, g=-15.885\n",
      ">1, 92/896, d=-10.687, g=20.670\n",
      ">1, 93/896, d=-17.092, g=30.572\n",
      ">1, 94/896, d=-12.621, g=-35.389\n",
      ">1, 95/896, d=-12.312, g=-29.357\n",
      ">1, 96/896, d=-11.705, g=22.744\n",
      ">1, 97/896, d=-11.449, g=34.958\n",
      ">1, 98/896, d=-12.516, g=42.608\n",
      ">1, 99/896, d=-13.687, g=32.224\n",
      ">1, 100/896, d=-13.189, g=45.654\n",
      ">1, 101/896, d=-12.017, g=-0.798\n",
      ">1, 102/896, d=-8.377, g=-10.373\n",
      ">1, 103/896, d=-12.497, g=-16.245\n",
      ">1, 104/896, d=-15.817, g=-24.929\n",
      ">1, 105/896, d=-12.098, g=1.988\n",
      ">1, 106/896, d=-11.806, g=-19.217\n",
      ">1, 107/896, d=-7.752, g=29.447\n",
      ">1, 108/896, d=-8.610, g=7.720\n",
      ">1, 109/896, d=-10.676, g=29.436\n",
      ">1, 110/896, d=-10.305, g=30.730\n",
      ">1, 111/896, d=-10.540, g=-2.105\n",
      ">1, 112/896, d=-13.216, g=62.310\n",
      ">1, 113/896, d=-11.415, g=0.723\n",
      ">1, 114/896, d=-10.366, g=-13.176\n",
      ">1, 115/896, d=-10.952, g=4.188\n",
      ">1, 116/896, d=-11.250, g=-2.711\n",
      ">1, 117/896, d=-11.064, g=-4.593\n",
      ">1, 118/896, d=-10.963, g=7.205\n",
      ">1, 119/896, d=-12.626, g=40.435\n",
      ">1, 120/896, d=-9.030, g=-24.825\n",
      ">1, 121/896, d=-12.884, g=-32.064\n",
      ">1, 122/896, d=-10.960, g=-4.812\n",
      ">1, 123/896, d=-10.552, g=40.525\n",
      ">1, 124/896, d=-12.199, g=31.387\n",
      ">1, 125/896, d=-9.161, g=-14.726\n",
      ">1, 126/896, d=-9.978, g=-22.539\n",
      ">1, 127/896, d=-12.244, g=3.777\n",
      ">1, 128/896, d=-11.615, g=3.498\n",
      ">1, 129/896, d=-14.145, g=23.324\n",
      ">1, 130/896, d=-7.726, g=-13.126\n",
      ">1, 131/896, d=-9.761, g=-9.064\n",
      ">1, 132/896, d=-12.186, g=28.057\n",
      ">1, 133/896, d=-10.399, g=20.358\n",
      ">1, 134/896, d=-10.443, g=3.449\n",
      ">1, 135/896, d=-10.988, g=-4.088\n",
      ">1, 136/896, d=-12.931, g=20.919\n",
      ">1, 137/896, d=-12.926, g=-38.848\n",
      ">1, 138/896, d=-12.431, g=-9.587\n",
      ">1, 139/896, d=-9.207, g=5.833\n",
      ">1, 140/896, d=-8.783, g=-1.221\n",
      ">1, 141/896, d=-10.290, g=3.129\n",
      ">1, 142/896, d=-10.821, g=11.358\n",
      ">1, 143/896, d=-9.733, g=61.707\n",
      ">1, 144/896, d=-10.069, g=19.435\n",
      ">1, 145/896, d=-12.829, g=-39.989\n",
      ">1, 146/896, d=-10.132, g=-16.343\n",
      ">1, 147/896, d=-11.188, g=1.939\n",
      ">1, 148/896, d=-11.693, g=16.790\n",
      ">1, 149/896, d=-13.193, g=-13.465\n",
      ">1, 150/896, d=-13.404, g=-24.121\n",
      ">1, 151/896, d=-9.516, g=8.201\n",
      ">1, 152/896, d=-10.695, g=23.930\n",
      ">1, 153/896, d=-9.295, g=10.464\n",
      ">1, 154/896, d=-8.372, g=9.293\n",
      ">1, 155/896, d=-9.785, g=15.279\n",
      ">1, 156/896, d=-9.015, g=29.671\n",
      ">1, 157/896, d=-8.582, g=-0.410\n",
      ">1, 158/896, d=-10.263, g=-4.999\n",
      ">1, 159/896, d=-9.871, g=-18.723\n",
      ">1, 160/896, d=-12.750, g=16.127\n",
      ">1, 161/896, d=-8.596, g=0.868\n",
      ">1, 162/896, d=-11.865, g=-33.872\n",
      ">1, 163/896, d=-8.412, g=0.353\n",
      ">1, 164/896, d=-10.213, g=9.982\n",
      ">1, 165/896, d=-9.581, g=23.212\n",
      ">1, 166/896, d=-10.077, g=-15.802\n",
      ">1, 167/896, d=-9.291, g=-18.897\n",
      ">1, 168/896, d=-10.549, g=-4.926\n",
      ">1, 169/896, d=-7.414, g=-8.885\n",
      ">1, 170/896, d=-11.444, g=10.259\n",
      ">1, 171/896, d=-11.935, g=16.604\n",
      ">1, 172/896, d=-13.237, g=10.517\n",
      ">1, 173/896, d=-8.769, g=41.290\n",
      ">1, 174/896, d=-9.562, g=20.151\n",
      ">1, 175/896, d=-9.046, g=9.701\n",
      ">1, 176/896, d=-12.128, g=-13.647\n",
      ">1, 177/896, d=-12.812, g=-8.526\n",
      ">1, 178/896, d=-12.715, g=-23.588\n",
      ">1, 179/896, d=-10.273, g=-0.769\n",
      ">1, 180/896, d=-8.899, g=-18.204\n",
      ">1, 181/896, d=-9.404, g=-1.184\n",
      ">1, 182/896, d=-7.313, g=-12.480\n",
      ">1, 183/896, d=-11.588, g=15.226\n",
      ">1, 184/896, d=-9.955, g=19.366\n",
      ">1, 185/896, d=-15.077, g=55.111\n",
      ">1, 186/896, d=-8.937, g=-17.499\n",
      ">1, 187/896, d=-12.446, g=-47.985\n",
      ">1, 188/896, d=-11.517, g=-0.251\n",
      ">1, 189/896, d=-12.972, g=-13.915\n",
      ">1, 190/896, d=-13.192, g=-20.271\n",
      ">1, 191/896, d=-13.519, g=4.595\n",
      ">1, 192/896, d=-12.062, g=19.671\n",
      ">1, 193/896, d=-10.451, g=3.092\n",
      ">1, 194/896, d=-8.747, g=60.214\n",
      ">1, 195/896, d=-7.573, g=15.892\n",
      ">1, 196/896, d=-8.977, g=18.945\n",
      ">1, 197/896, d=-9.721, g=1.463\n",
      ">1, 198/896, d=-8.785, g=-10.475\n",
      ">1, 199/896, d=-11.207, g=-42.754\n",
      ">1, 200/896, d=-12.053, g=-32.011\n",
      ">1, 201/896, d=-11.365, g=-26.974\n",
      ">1, 202/896, d=-8.078, g=-9.357\n",
      ">1, 203/896, d=-9.815, g=20.940\n",
      ">1, 204/896, d=-13.233, g=97.269\n",
      ">1, 205/896, d=-10.785, g=45.299\n",
      ">1, 206/896, d=-8.198, g=7.512\n",
      ">1, 207/896, d=-8.718, g=-8.739\n",
      ">1, 208/896, d=-8.049, g=-28.079\n",
      ">1, 209/896, d=-7.780, g=-38.045\n",
      ">1, 210/896, d=-11.718, g=-41.581\n",
      ">1, 211/896, d=-10.559, g=-30.511\n",
      ">1, 212/896, d=-7.769, g=-0.854\n",
      ">1, 213/896, d=-9.251, g=18.899\n",
      ">1, 214/896, d=-6.912, g=18.124\n",
      ">1, 215/896, d=-10.215, g=14.154\n",
      ">1, 216/896, d=-10.919, g=-37.035\n",
      ">1, 217/896, d=-12.256, g=-41.335\n",
      ">1, 218/896, d=-9.136, g=4.710\n",
      ">1, 219/896, d=-7.682, g=35.940\n",
      ">1, 220/896, d=-9.692, g=13.755\n",
      ">1, 221/896, d=-7.550, g=13.536\n",
      ">1, 222/896, d=-7.435, g=-7.701\n",
      ">1, 223/896, d=-8.112, g=17.421\n",
      ">1, 224/896, d=-8.832, g=15.853\n",
      ">1, 225/896, d=-10.154, g=14.722\n",
      ">1, 226/896, d=-8.795, g=-15.661\n",
      ">1, 227/896, d=-10.124, g=-28.918\n",
      ">1, 228/896, d=-8.152, g=-7.464\n",
      ">1, 229/896, d=-7.411, g=-4.421\n",
      ">1, 230/896, d=-9.170, g=4.641\n",
      ">1, 231/896, d=-10.635, g=-7.360\n",
      ">1, 232/896, d=-12.569, g=-16.506\n",
      ">1, 233/896, d=-10.365, g=-5.742\n",
      ">1, 234/896, d=-9.687, g=21.403\n",
      ">1, 235/896, d=-10.182, g=18.857\n",
      ">1, 236/896, d=-5.598, g=0.643\n",
      ">1, 237/896, d=-7.195, g=-5.997\n",
      ">1, 238/896, d=-7.017, g=-8.662\n",
      ">1, 239/896, d=-12.048, g=-13.399\n",
      ">1, 240/896, d=-14.598, g=-13.094\n",
      ">1, 241/896, d=-8.086, g=3.039\n",
      ">1, 242/896, d=-7.202, g=32.194\n",
      ">1, 243/896, d=-8.703, g=44.470\n",
      ">1, 244/896, d=-6.727, g=46.000\n",
      ">1, 245/896, d=-7.035, g=22.377\n",
      ">1, 246/896, d=-9.204, g=3.385\n",
      ">1, 247/896, d=-11.066, g=-12.022\n",
      ">1, 248/896, d=-7.058, g=-13.365\n",
      ">1, 249/896, d=-6.839, g=-5.351\n",
      ">1, 250/896, d=-9.432, g=-10.040\n",
      ">1, 251/896, d=-9.007, g=-18.095\n",
      ">1, 252/896, d=-6.634, g=-32.878\n",
      ">1, 253/896, d=-13.280, g=-47.424\n",
      ">1, 254/896, d=-6.408, g=-42.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 255/896, d=-9.698, g=11.758\n",
      ">1, 256/896, d=-12.269, g=55.500\n",
      ">1, 257/896, d=-7.910, g=25.165\n",
      ">1, 258/896, d=-5.355, g=16.239\n",
      ">1, 259/896, d=-7.824, g=9.414\n",
      ">1, 260/896, d=-5.747, g=-5.106\n",
      ">1, 261/896, d=-8.570, g=-5.959\n",
      ">1, 262/896, d=-11.708, g=-12.223\n",
      ">1, 263/896, d=-10.384, g=-16.965\n",
      ">1, 264/896, d=-7.827, g=9.525\n",
      ">1, 265/896, d=-7.076, g=37.940\n",
      ">1, 266/896, d=-6.182, g=41.520\n",
      ">1, 267/896, d=-7.494, g=35.055\n",
      ">1, 268/896, d=-7.539, g=-7.262\n",
      ">1, 269/896, d=-9.765, g=-29.285\n",
      ">1, 270/896, d=-5.477, g=-24.901\n",
      ">1, 271/896, d=-8.841, g=-23.082\n",
      ">1, 272/896, d=-9.628, g=-7.856\n",
      ">1, 273/896, d=-10.272, g=0.601\n",
      ">1, 274/896, d=-5.251, g=4.307\n",
      ">1, 275/896, d=-6.957, g=-4.214\n",
      ">1, 276/896, d=-9.065, g=-18.075\n",
      ">1, 277/896, d=-6.577, g=-24.266\n",
      ">1, 278/896, d=-7.766, g=-14.705\n",
      ">1, 279/896, d=-7.421, g=-1.416\n",
      ">1, 280/896, d=-5.735, g=11.716\n",
      ">1, 281/896, d=-8.487, g=24.515\n",
      ">1, 282/896, d=-8.359, g=31.979\n",
      ">1, 283/896, d=-5.184, g=20.311\n",
      ">1, 284/896, d=-8.298, g=-16.185\n",
      ">1, 285/896, d=-7.474, g=-26.940\n",
      ">1, 286/896, d=-9.390, g=-32.108\n",
      ">1, 287/896, d=-8.138, g=-30.063\n",
      ">1, 288/896, d=-4.776, g=7.671\n",
      ">1, 289/896, d=-8.062, g=25.772\n",
      ">1, 290/896, d=-6.941, g=16.658\n",
      ">1, 291/896, d=-10.970, g=21.293\n",
      ">1, 292/896, d=-5.718, g=1.782\n",
      ">1, 293/896, d=-4.738, g=-19.562\n",
      ">1, 294/896, d=-9.682, g=-42.052\n",
      ">1, 295/896, d=-10.262, g=-48.867\n",
      ">1, 296/896, d=-7.338, g=-26.405\n",
      ">1, 297/896, d=-5.766, g=1.390\n",
      ">1, 298/896, d=-6.823, g=20.936\n",
      ">1, 299/896, d=-8.536, g=33.512\n",
      ">1, 300/896, d=-8.834, g=34.570\n",
      ">1, 301/896, d=-7.995, g=-14.110\n",
      ">1, 302/896, d=-11.475, g=-41.992\n",
      ">1, 303/896, d=-9.938, g=-26.994\n",
      ">1, 304/896, d=-8.662, g=-34.368\n",
      ">1, 305/896, d=-5.033, g=-8.806\n",
      ">1, 306/896, d=-7.989, g=15.503\n",
      ">1, 307/896, d=-9.879, g=37.770\n",
      ">1, 308/896, d=-10.584, g=22.890\n",
      ">1, 309/896, d=-6.512, g=-29.983\n",
      ">1, 310/896, d=-6.379, g=-53.873\n",
      ">1, 311/896, d=-10.833, g=-32.500\n",
      ">1, 312/896, d=-8.997, g=-12.229\n",
      ">1, 313/896, d=-10.613, g=23.912\n",
      ">1, 314/896, d=-9.093, g=50.256\n",
      ">1, 315/896, d=-6.618, g=17.284\n",
      ">1, 316/896, d=-5.082, g=15.083\n",
      ">1, 317/896, d=-5.609, g=1.050\n",
      ">1, 318/896, d=-8.466, g=-0.185\n",
      ">1, 319/896, d=-4.744, g=-26.409\n",
      ">1, 320/896, d=-10.064, g=-13.498\n",
      ">1, 321/896, d=-9.663, g=-4.550\n",
      ">1, 322/896, d=-7.297, g=9.602\n",
      ">1, 323/896, d=-8.747, g=18.008\n",
      ">1, 324/896, d=-5.333, g=26.781\n",
      ">1, 325/896, d=-10.321, g=27.726\n",
      ">1, 326/896, d=-9.104, g=4.677\n",
      ">1, 327/896, d=-7.372, g=-48.456\n",
      ">1, 328/896, d=-8.300, g=-67.114\n",
      ">1, 329/896, d=-5.414, g=-45.994\n",
      ">1, 330/896, d=-10.152, g=-26.027\n",
      ">1, 331/896, d=-9.597, g=-8.878\n",
      ">1, 332/896, d=-9.461, g=6.992\n",
      ">1, 333/896, d=-8.665, g=16.901\n",
      ">1, 334/896, d=-5.356, g=42.660\n",
      ">1, 335/896, d=-5.310, g=17.325\n",
      ">1, 336/896, d=-10.826, g=-23.107\n",
      ">1, 337/896, d=-8.399, g=-41.519\n",
      ">1, 338/896, d=-9.855, g=-41.437\n",
      ">1, 339/896, d=-11.750, g=2.216\n",
      ">1, 340/896, d=-6.063, g=32.460\n",
      ">1, 341/896, d=-8.428, g=41.667\n",
      ">1, 342/896, d=-3.801, g=21.631\n",
      ">1, 343/896, d=-9.150, g=-13.956\n",
      ">1, 344/896, d=-4.646, g=-12.257\n",
      ">1, 345/896, d=-6.188, g=-28.969\n",
      ">1, 346/896, d=-8.243, g=-28.055\n",
      ">1, 347/896, d=-7.239, g=-12.139\n",
      ">1, 348/896, d=-5.314, g=-6.626\n",
      ">1, 349/896, d=-7.906, g=6.207\n",
      ">1, 350/896, d=-8.616, g=38.593\n",
      ">1, 351/896, d=-5.459, g=39.355\n",
      ">1, 352/896, d=-7.809, g=16.877\n",
      ">1, 353/896, d=-6.930, g=-17.563\n",
      ">1, 354/896, d=-10.611, g=-49.371\n",
      ">1, 355/896, d=-9.083, g=-66.348\n",
      ">1, 356/896, d=-8.423, g=-56.895\n",
      ">1, 357/896, d=-6.081, g=-28.737\n",
      ">1, 358/896, d=-8.149, g=5.900\n",
      ">1, 359/896, d=-9.028, g=33.060\n",
      ">1, 360/896, d=-6.731, g=35.746\n",
      ">1, 361/896, d=-9.732, g=22.308\n",
      ">1, 362/896, d=-7.197, g=4.228\n",
      ">1, 363/896, d=-9.899, g=-17.748\n",
      ">1, 364/896, d=-6.112, g=-44.991\n",
      ">1, 365/896, d=-5.163, g=-51.174\n",
      ">1, 366/896, d=-3.284, g=-41.229\n",
      ">1, 367/896, d=-4.982, g=-13.236\n",
      ">1, 368/896, d=-7.917, g=-10.329\n",
      ">1, 369/896, d=-5.219, g=28.743\n",
      ">1, 370/896, d=-8.229, g=27.259\n",
      ">1, 371/896, d=-7.872, g=25.088\n",
      ">1, 372/896, d=-6.732, g=20.580\n",
      ">1, 373/896, d=-12.106, g=-11.714\n",
      ">1, 374/896, d=-11.994, g=-47.024\n",
      ">1, 375/896, d=-9.381, g=-34.190\n",
      ">1, 376/896, d=-5.189, g=-19.455\n",
      ">1, 377/896, d=-6.920, g=7.095\n",
      ">1, 378/896, d=-7.280, g=37.646\n",
      ">1, 379/896, d=-6.575, g=29.999\n",
      ">1, 380/896, d=-7.944, g=12.251\n",
      ">1, 381/896, d=-6.377, g=-11.453\n",
      ">1, 382/896, d=-7.571, g=-20.082\n",
      ">1, 383/896, d=-12.016, g=-27.124\n",
      ">1, 384/896, d=-6.580, g=-27.116\n",
      ">1, 385/896, d=-7.361, g=-42.972\n",
      ">1, 386/896, d=-8.348, g=-43.287\n",
      ">1, 387/896, d=-6.729, g=-16.034\n",
      ">1, 388/896, d=-4.513, g=15.705\n",
      ">1, 389/896, d=-8.533, g=33.851\n",
      ">1, 390/896, d=-7.561, g=44.955\n",
      ">1, 391/896, d=-7.486, g=17.365\n",
      ">1, 392/896, d=-6.025, g=-12.381\n",
      ">1, 393/896, d=-6.393, g=-50.735\n",
      ">1, 394/896, d=-9.903, g=-61.372\n",
      ">1, 395/896, d=-11.493, g=-67.005\n",
      ">1, 396/896, d=-5.048, g=-51.761\n",
      ">1, 397/896, d=-7.542, g=-26.551\n",
      ">1, 398/896, d=-10.268, g=1.145\n",
      ">1, 399/896, d=-8.660, g=59.794\n",
      ">1, 400/896, d=-8.639, g=55.584\n",
      ">1, 401/896, d=-10.296, g=64.156\n",
      ">1, 402/896, d=-7.183, g=20.178\n",
      ">1, 403/896, d=-7.104, g=-27.384\n",
      ">1, 404/896, d=-7.189, g=-56.200\n",
      ">1, 405/896, d=-10.777, g=-80.794\n",
      ">1, 406/896, d=-9.773, g=-70.911\n",
      ">1, 407/896, d=-12.044, g=-48.137\n",
      ">1, 408/896, d=-3.814, g=-10.975\n",
      ">1, 409/896, d=-7.252, g=0.877\n",
      ">1, 410/896, d=-6.469, g=16.539\n",
      ">1, 411/896, d=-8.303, g=22.399\n",
      ">1, 412/896, d=-10.059, g=-17.348\n",
      ">1, 413/896, d=-6.026, g=-9.057\n",
      ">1, 414/896, d=-6.409, g=-7.480\n",
      ">1, 415/896, d=-4.082, g=26.840\n",
      ">1, 416/896, d=-5.871, g=38.258\n",
      ">1, 417/896, d=-5.042, g=-6.419\n",
      ">1, 418/896, d=-9.170, g=-31.991\n",
      ">1, 419/896, d=-9.957, g=-47.778\n",
      ">1, 420/896, d=-11.896, g=-38.798\n",
      ">1, 421/896, d=-7.334, g=-0.792\n",
      ">1, 422/896, d=-10.070, g=42.459\n",
      ">1, 423/896, d=-7.914, g=51.649\n",
      ">1, 424/896, d=-6.009, g=29.063\n",
      ">1, 425/896, d=-8.654, g=5.975\n",
      ">1, 426/896, d=-4.660, g=-31.811\n",
      ">1, 427/896, d=-8.765, g=-57.860\n",
      ">1, 428/896, d=-9.429, g=-61.561\n",
      ">1, 429/896, d=-6.754, g=-31.768\n",
      ">1, 430/896, d=-9.327, g=-6.463\n",
      ">1, 431/896, d=-7.559, g=18.462\n",
      ">1, 432/896, d=-9.733, g=55.043\n",
      ">1, 433/896, d=-8.585, g=26.099\n",
      ">1, 434/896, d=-5.989, g=-3.464\n",
      ">1, 435/896, d=-5.392, g=-18.766\n",
      ">1, 436/896, d=-6.857, g=-39.928\n",
      ">1, 437/896, d=-10.065, g=-46.102\n",
      ">1, 438/896, d=-8.457, g=-37.188\n",
      ">1, 439/896, d=-8.457, g=-15.106\n",
      ">1, 440/896, d=-9.275, g=12.075\n",
      ">1, 441/896, d=-8.788, g=22.334\n",
      ">1, 442/896, d=-5.401, g=26.209\n",
      ">1, 443/896, d=-7.514, g=13.751\n",
      ">1, 444/896, d=-6.074, g=16.833\n",
      ">1, 445/896, d=-5.377, g=6.511\n",
      ">1, 446/896, d=-3.794, g=-7.581\n",
      ">1, 447/896, d=-6.798, g=-10.987\n",
      ">1, 448/896, d=-8.486, g=-38.009\n",
      ">1, 449/896, d=-9.371, g=-35.785\n",
      ">1, 450/896, d=-8.711, g=-34.975\n",
      ">1, 451/896, d=-7.759, g=-8.865\n",
      ">1, 452/896, d=-7.801, g=18.714\n",
      ">1, 453/896, d=-6.327, g=32.942\n",
      ">1, 454/896, d=-9.207, g=19.612\n",
      ">1, 455/896, d=-7.123, g=11.279\n",
      ">1, 456/896, d=-5.937, g=-3.139\n",
      ">1, 457/896, d=-7.886, g=-7.660\n",
      ">1, 458/896, d=-7.639, g=-34.195\n",
      ">1, 459/896, d=-4.891, g=-30.762\n",
      ">1, 460/896, d=-9.482, g=-31.781\n",
      ">1, 461/896, d=-7.876, g=-15.417\n",
      ">1, 462/896, d=-3.760, g=29.282\n",
      ">1, 463/896, d=-10.061, g=53.040\n",
      ">1, 464/896, d=-7.198, g=39.738\n",
      ">1, 465/896, d=-6.262, g=24.309\n",
      ">1, 466/896, d=-6.554, g=1.186\n",
      ">1, 467/896, d=-7.949, g=-21.459\n",
      ">1, 468/896, d=-10.894, g=-43.224\n",
      ">1, 469/896, d=-6.837, g=-46.125\n",
      ">1, 470/896, d=-7.728, g=-44.724\n",
      ">1, 471/896, d=-6.088, g=-9.180\n",
      ">1, 472/896, d=-5.406, g=12.947\n",
      ">1, 473/896, d=-11.397, g=23.363\n",
      ">1, 474/896, d=-6.942, g=29.116\n",
      ">1, 475/896, d=-2.248, g=5.870\n",
      ">1, 476/896, d=-10.344, g=15.258\n",
      ">1, 477/896, d=-10.052, g=0.333\n",
      ">1, 478/896, d=-9.324, g=-14.869\n",
      ">1, 479/896, d=-6.139, g=-42.316\n",
      ">1, 480/896, d=-7.441, g=-44.332\n",
      ">1, 481/896, d=-6.211, g=-3.291\n",
      ">1, 482/896, d=-7.208, g=19.521\n",
      ">1, 483/896, d=-8.207, g=43.701\n",
      ">1, 484/896, d=-9.255, g=46.698\n",
      ">1, 485/896, d=-7.729, g=45.451\n",
      ">1, 486/896, d=-9.135, g=1.664\n",
      ">1, 487/896, d=-6.119, g=-36.011\n",
      ">1, 488/896, d=-8.364, g=-48.873\n",
      ">1, 489/896, d=-5.851, g=-38.259\n",
      ">1, 490/896, d=-6.393, g=-36.569\n",
      ">1, 491/896, d=-3.645, g=-12.546\n",
      ">1, 492/896, d=-5.996, g=2.548\n",
      ">1, 493/896, d=-10.324, g=3.944\n",
      ">1, 494/896, d=-5.062, g=27.141\n",
      ">1, 495/896, d=-5.483, g=37.097\n",
      ">1, 496/896, d=-7.642, g=46.798\n",
      ">1, 497/896, d=-6.149, g=4.319\n",
      ">1, 498/896, d=-6.247, g=-25.788\n",
      ">1, 499/896, d=-9.543, g=-36.681\n",
      ">1, 500/896, d=-7.347, g=-30.078\n",
      ">1, 501/896, d=-4.728, g=-13.629\n",
      ">1, 502/896, d=-7.151, g=-2.323\n",
      ">1, 503/896, d=-3.861, g=21.934\n",
      ">1, 504/896, d=-7.303, g=43.504\n",
      ">1, 505/896, d=-4.407, g=22.426\n",
      ">1, 506/896, d=-5.808, g=6.732\n",
      ">1, 507/896, d=-8.964, g=-4.645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 508/896, d=-7.589, g=-11.659\n",
      ">1, 509/896, d=-6.459, g=-18.197\n",
      ">1, 510/896, d=-9.947, g=-13.948\n",
      ">1, 511/896, d=-5.858, g=3.052\n",
      ">1, 512/896, d=-8.436, g=22.217\n",
      ">1, 513/896, d=-9.625, g=28.009\n",
      ">1, 514/896, d=-8.235, g=16.970\n",
      ">1, 515/896, d=-7.794, g=-18.831\n",
      ">1, 516/896, d=-8.060, g=-78.310\n",
      ">1, 517/896, d=-9.220, g=-20.942\n",
      ">1, 518/896, d=-7.345, g=2.369\n",
      ">1, 519/896, d=-5.788, g=23.602\n",
      ">1, 520/896, d=-8.427, g=30.921\n",
      ">1, 521/896, d=-7.600, g=45.165\n",
      ">1, 522/896, d=-8.573, g=47.064\n",
      ">1, 523/896, d=-5.900, g=16.592\n",
      ">1, 524/896, d=-5.981, g=-1.475\n",
      ">1, 525/896, d=-8.050, g=-18.118\n",
      ">1, 526/896, d=-7.581, g=-34.497\n",
      ">1, 527/896, d=-6.897, g=-31.811\n",
      ">1, 528/896, d=-5.828, g=-17.939\n",
      ">1, 529/896, d=-4.200, g=5.553\n",
      ">1, 530/896, d=-9.651, g=20.722\n",
      ">1, 531/896, d=-10.204, g=40.297\n",
      ">1, 532/896, d=-6.385, g=47.403\n",
      ">1, 533/896, d=-4.887, g=42.362\n",
      ">1, 534/896, d=-5.748, g=34.909\n",
      ">1, 535/896, d=-9.429, g=24.690\n",
      ">1, 536/896, d=-7.376, g=4.182\n",
      ">1, 537/896, d=-6.518, g=-3.398\n",
      ">1, 538/896, d=-8.623, g=-7.718\n",
      ">1, 539/896, d=-7.966, g=1.529\n",
      ">1, 540/896, d=-8.030, g=-8.297\n",
      ">1, 541/896, d=-8.207, g=-33.227\n",
      ">1, 542/896, d=-4.991, g=-23.448\n",
      ">1, 543/896, d=-6.372, g=-12.204\n",
      ">1, 544/896, d=-6.865, g=3.997\n",
      ">1, 545/896, d=-10.854, g=21.729\n",
      ">1, 546/896, d=-8.443, g=35.936\n",
      ">1, 547/896, d=-5.213, g=59.388\n",
      ">1, 548/896, d=-11.292, g=23.803\n",
      ">1, 549/896, d=-6.790, g=-12.267\n",
      ">1, 550/896, d=-12.684, g=-34.512\n",
      ">1, 551/896, d=-9.155, g=-13.514\n",
      ">1, 552/896, d=-4.541, g=-24.472\n",
      ">1, 553/896, d=-7.505, g=-3.945\n",
      ">1, 554/896, d=-9.934, g=5.192\n",
      ">1, 555/896, d=-8.308, g=30.382\n",
      ">1, 556/896, d=-5.798, g=7.793\n",
      ">1, 557/896, d=-7.226, g=-9.910\n",
      ">1, 558/896, d=-4.213, g=-7.677\n",
      ">1, 559/896, d=-8.146, g=-18.957\n",
      ">1, 560/896, d=-10.470, g=-9.520\n",
      ">1, 561/896, d=-8.324, g=-11.090\n",
      ">1, 562/896, d=-8.294, g=-3.579\n",
      ">1, 563/896, d=-9.062, g=3.301\n",
      ">1, 564/896, d=-6.690, g=14.387\n",
      ">1, 565/896, d=-7.814, g=31.421\n",
      ">1, 566/896, d=-9.946, g=37.806\n",
      ">1, 567/896, d=-6.056, g=19.991\n",
      ">1, 568/896, d=-7.279, g=16.226\n",
      ">1, 569/896, d=-7.527, g=6.711\n",
      ">1, 570/896, d=-9.209, g=-4.404\n",
      ">1, 571/896, d=-4.873, g=-22.021\n",
      ">1, 572/896, d=-7.538, g=-7.813\n",
      ">1, 573/896, d=-9.860, g=15.177\n",
      ">1, 574/896, d=-9.398, g=-4.352\n",
      ">1, 575/896, d=-3.809, g=2.055\n",
      ">1, 576/896, d=0.030, g=-19.857\n",
      ">1, 577/896, d=-2.234, g=-10.913\n",
      ">1, 578/896, d=-9.384, g=-14.225\n",
      ">1, 579/896, d=-15.038, g=-28.564\n",
      ">1, 580/896, d=-7.769, g=38.600\n",
      ">1, 581/896, d=-7.733, g=76.635\n",
      ">1, 582/896, d=-10.544, g=102.028\n",
      ">1, 583/896, d=-7.190, g=71.198\n",
      ">1, 584/896, d=-14.443, g=43.919\n",
      ">1, 585/896, d=-4.412, g=-1.708\n",
      ">1, 586/896, d=-5.656, g=-27.354\n",
      ">1, 587/896, d=-6.292, g=-55.451\n",
      ">1, 588/896, d=-5.722, g=-61.139\n",
      ">1, 589/896, d=-6.138, g=-48.741\n",
      ">1, 590/896, d=-4.503, g=-31.435\n",
      ">1, 591/896, d=-4.416, g=-18.431\n",
      ">1, 592/896, d=-10.816, g=-3.400\n",
      ">1, 593/896, d=-8.255, g=22.742\n",
      ">1, 594/896, d=-3.614, g=27.806\n",
      ">1, 595/896, d=-6.348, g=34.678\n",
      ">1, 596/896, d=-5.011, g=17.643\n",
      ">1, 597/896, d=-6.914, g=10.424\n",
      ">1, 598/896, d=-6.767, g=3.001\n",
      ">1, 599/896, d=-9.006, g=-0.433\n",
      ">1, 600/896, d=-7.443, g=-1.010\n",
      ">1, 601/896, d=-6.733, g=-9.004\n",
      ">1, 602/896, d=-7.590, g=-19.240\n",
      ">1, 603/896, d=-8.704, g=-23.774\n",
      ">1, 604/896, d=-7.823, g=-19.011\n",
      ">1, 605/896, d=-9.058, g=-18.614\n",
      ">1, 606/896, d=-6.436, g=12.514\n",
      ">1, 607/896, d=-10.119, g=42.656\n",
      ">1, 608/896, d=-5.925, g=46.745\n",
      ">1, 609/896, d=-6.862, g=37.401\n",
      ">1, 610/896, d=-7.649, g=32.211\n",
      ">1, 611/896, d=-6.905, g=30.984\n",
      ">1, 612/896, d=-7.803, g=45.289\n",
      ">1, 613/896, d=-3.806, g=10.559\n",
      ">1, 614/896, d=-6.189, g=-0.163\n",
      ">1, 615/896, d=-8.778, g=13.944\n",
      ">1, 616/896, d=-9.137, g=-5.132\n",
      ">1, 617/896, d=-6.649, g=-7.412\n",
      ">1, 618/896, d=-10.143, g=-19.939\n",
      ">1, 619/896, d=-6.676, g=-2.020\n",
      ">1, 620/896, d=-8.812, g=14.717\n",
      ">1, 621/896, d=-5.305, g=14.880\n",
      ">1, 622/896, d=-8.102, g=17.766\n",
      ">1, 623/896, d=-7.732, g=15.315\n",
      ">1, 624/896, d=-2.104, g=4.708\n",
      ">1, 625/896, d=-7.117, g=2.329\n",
      ">1, 626/896, d=-10.475, g=-3.694\n",
      ">1, 627/896, d=-6.541, g=24.076\n",
      ">1, 628/896, d=-6.936, g=31.185\n",
      ">1, 629/896, d=-9.962, g=10.518\n",
      ">1, 630/896, d=-11.468, g=33.771\n",
      ">1, 631/896, d=-2.919, g=27.849\n",
      ">1, 632/896, d=-8.918, g=2.921\n",
      ">1, 633/896, d=-7.441, g=2.430\n",
      ">1, 634/896, d=-9.466, g=6.508\n",
      ">1, 635/896, d=-9.017, g=8.159\n",
      ">1, 636/896, d=-8.079, g=2.752\n",
      ">1, 637/896, d=-6.524, g=14.149\n",
      ">1, 638/896, d=-12.385, g=16.828\n",
      ">1, 639/896, d=-12.804, g=-3.246\n",
      ">1, 640/896, d=-7.979, g=12.447\n",
      ">1, 641/896, d=-8.038, g=-4.391\n",
      ">1, 642/896, d=-12.282, g=-0.100\n",
      ">1, 643/896, d=-5.593, g=17.471\n",
      ">1, 644/896, d=-7.177, g=31.049\n",
      ">1, 645/896, d=-12.866, g=19.108\n",
      ">1, 646/896, d=-6.347, g=4.755\n",
      ">1, 647/896, d=-8.199, g=7.852\n",
      ">1, 648/896, d=-9.272, g=6.898\n",
      ">1, 649/896, d=-9.457, g=18.848\n",
      ">1, 650/896, d=-9.160, g=35.341\n",
      ">1, 651/896, d=-8.761, g=41.079\n",
      ">1, 652/896, d=-10.278, g=45.783\n",
      ">1, 653/896, d=-6.281, g=39.074\n",
      ">1, 654/896, d=-7.348, g=40.096\n",
      ">1, 655/896, d=-10.678, g=15.438\n",
      ">1, 656/896, d=-6.250, g=19.746\n",
      ">1, 657/896, d=-7.725, g=-1.153\n",
      ">1, 658/896, d=-9.565, g=-12.908\n",
      ">1, 659/896, d=-7.399, g=-9.475\n",
      ">1, 660/896, d=-6.486, g=-29.973\n",
      ">1, 661/896, d=-7.021, g=-23.659\n",
      ">1, 662/896, d=-6.913, g=8.369\n",
      ">1, 663/896, d=-10.939, g=35.058\n",
      ">1, 664/896, d=-3.813, g=43.695\n",
      ">1, 665/896, d=-7.631, g=29.214\n",
      ">1, 666/896, d=-6.925, g=25.259\n",
      ">1, 667/896, d=-6.186, g=6.126\n",
      ">1, 668/896, d=-7.064, g=2.903\n",
      ">1, 669/896, d=-14.414, g=8.715\n",
      ">1, 670/896, d=-4.820, g=13.062\n",
      ">1, 671/896, d=-7.327, g=16.018\n",
      ">1, 672/896, d=-5.329, g=15.896\n",
      ">1, 673/896, d=-3.171, g=13.308\n",
      ">1, 674/896, d=-5.560, g=0.430\n",
      ">1, 675/896, d=-10.870, g=-0.164\n",
      ">1, 676/896, d=-6.895, g=14.923\n",
      ">1, 677/896, d=-8.902, g=20.108\n",
      ">1, 678/896, d=-9.974, g=18.986\n",
      ">1, 679/896, d=-0.616, g=23.552\n",
      ">1, 680/896, d=-0.616, g=24.309\n",
      ">1, 681/896, d=-11.787, g=11.304\n",
      ">1, 682/896, d=-9.507, g=-2.144\n",
      ">1, 683/896, d=-10.504, g=-16.512\n",
      ">1, 684/896, d=-8.791, g=-17.759\n",
      ">1, 685/896, d=-5.932, g=1.335\n",
      ">1, 686/896, d=-12.953, g=39.692\n",
      ">1, 687/896, d=-6.396, g=52.719\n",
      ">1, 688/896, d=-8.383, g=39.874\n",
      ">1, 689/896, d=-11.005, g=30.519\n",
      ">1, 690/896, d=-2.482, g=41.281\n",
      ">1, 691/896, d=-7.451, g=31.189\n",
      ">1, 692/896, d=-8.430, g=30.064\n",
      ">1, 693/896, d=-6.022, g=10.824\n",
      ">1, 694/896, d=-10.252, g=5.903\n",
      ">1, 695/896, d=-5.162, g=-3.818\n",
      ">1, 696/896, d=-5.705, g=-7.455\n",
      ">1, 697/896, d=-9.214, g=-12.999\n",
      ">1, 698/896, d=-9.197, g=-5.298\n",
      ">1, 699/896, d=-11.963, g=3.275\n",
      ">1, 700/896, d=-2.745, g=-1.615\n",
      ">1, 701/896, d=-8.660, g=3.732\n",
      ">1, 702/896, d=-8.308, g=-1.323\n",
      ">1, 703/896, d=-13.493, g=-2.930\n",
      ">1, 704/896, d=-1.622, g=-10.339\n",
      ">1, 705/896, d=-14.621, g=-4.594\n",
      ">1, 706/896, d=-7.580, g=12.549\n",
      ">1, 707/896, d=-13.632, g=45.818\n",
      ">1, 708/896, d=-1.991, g=42.283\n",
      ">1, 709/896, d=-1.969, g=16.548\n",
      ">1, 710/896, d=-5.845, g=37.056\n",
      ">1, 711/896, d=-10.562, g=55.077\n",
      ">1, 712/896, d=-4.332, g=50.332\n",
      ">1, 713/896, d=-11.025, g=53.759\n",
      ">1, 714/896, d=-6.467, g=42.249\n",
      ">1, 715/896, d=-4.228, g=27.537\n",
      ">1, 716/896, d=-3.927, g=17.669\n",
      ">1, 717/896, d=-5.845, g=10.214\n",
      ">1, 718/896, d=-8.254, g=15.614\n",
      ">1, 719/896, d=-7.810, g=20.154\n",
      ">1, 720/896, d=-7.471, g=16.008\n",
      ">1, 721/896, d=-6.178, g=10.367\n",
      ">1, 722/896, d=-6.365, g=7.371\n",
      ">1, 723/896, d=-3.379, g=12.539\n",
      ">1, 724/896, d=-9.858, g=23.349\n",
      ">1, 725/896, d=-6.696, g=31.313\n",
      ">1, 726/896, d=-8.507, g=43.088\n",
      ">1, 727/896, d=-11.774, g=43.460\n",
      ">1, 728/896, d=-11.500, g=40.905\n",
      ">1, 729/896, d=-5.661, g=29.955\n",
      ">1, 730/896, d=-10.477, g=18.233\n",
      ">1, 731/896, d=-10.022, g=15.841\n",
      ">1, 732/896, d=-2.082, g=5.225\n",
      ">1, 733/896, d=-5.122, g=1.608\n",
      ">1, 734/896, d=-7.429, g=-17.817\n",
      ">1, 735/896, d=-8.801, g=18.604\n",
      ">1, 736/896, d=-1.255, g=39.097\n",
      ">1, 737/896, d=-7.991, g=43.024\n",
      ">1, 738/896, d=-3.708, g=50.721\n",
      ">1, 739/896, d=-1.345, g=48.498\n",
      ">1, 740/896, d=-3.519, g=34.452\n",
      ">1, 741/896, d=-6.349, g=19.490\n",
      ">1, 742/896, d=-6.578, g=14.283\n",
      ">1, 743/896, d=-9.627, g=22.493\n",
      ">1, 744/896, d=-10.250, g=29.569\n",
      ">1, 745/896, d=-2.320, g=17.725\n",
      ">1, 746/896, d=-9.190, g=10.925\n",
      ">1, 747/896, d=-9.945, g=53.678\n",
      ">1, 748/896, d=-4.583, g=80.818\n",
      ">1, 749/896, d=0.085, g=49.394\n",
      ">1, 750/896, d=-7.509, g=66.791\n",
      ">1, 751/896, d=2.273, g=63.399\n",
      ">1, 752/896, d=-6.526, g=54.248\n",
      ">1, 753/896, d=-7.352, g=41.774\n",
      ">1, 754/896, d=-7.881, g=28.637\n",
      ">1, 755/896, d=-3.208, g=-0.015\n",
      ">1, 756/896, d=-5.226, g=-10.337\n",
      ">1, 757/896, d=-7.140, g=-16.886\n",
      ">1, 758/896, d=-3.222, g=-12.743\n",
      ">1, 759/896, d=-5.617, g=-17.967\n",
      ">1, 760/896, d=-5.568, g=-11.148\n",
      ">1, 761/896, d=-6.525, g=-17.361\n",
      ">1, 762/896, d=-7.695, g=-4.404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 763/896, d=-6.842, g=16.061\n",
      ">1, 764/896, d=-9.778, g=25.688\n",
      ">1, 765/896, d=-10.453, g=43.540\n",
      ">1, 766/896, d=-8.912, g=50.100\n",
      ">1, 767/896, d=-1.064, g=36.174\n",
      ">1, 768/896, d=-5.239, g=39.866\n",
      ">1, 769/896, d=-5.006, g=40.385\n",
      ">1, 770/896, d=-8.230, g=41.306\n",
      ">1, 771/896, d=-8.043, g=48.623\n",
      ">1, 772/896, d=-8.852, g=39.591\n",
      ">1, 773/896, d=-3.556, g=26.758\n",
      ">1, 774/896, d=-8.762, g=8.667\n",
      ">1, 775/896, d=-5.758, g=4.601\n",
      ">1, 776/896, d=-4.557, g=3.919\n",
      ">1, 777/896, d=-6.884, g=9.670\n",
      ">1, 778/896, d=-6.313, g=9.993\n",
      ">1, 779/896, d=-6.266, g=6.167\n",
      ">1, 780/896, d=-9.290, g=15.108\n",
      ">1, 781/896, d=-6.830, g=13.499\n",
      ">1, 782/896, d=-4.607, g=6.315\n",
      ">1, 783/896, d=-5.439, g=1.497\n",
      ">1, 784/896, d=-7.687, g=-2.308\n",
      ">1, 785/896, d=-9.847, g=-12.327\n",
      ">1, 786/896, d=-7.541, g=-16.292\n",
      ">1, 787/896, d=-8.501, g=-18.010\n",
      ">1, 788/896, d=-2.339, g=-29.571\n",
      ">1, 789/896, d=-4.466, g=-36.829\n",
      ">1, 790/896, d=-7.037, g=-29.710\n",
      ">1, 791/896, d=-9.700, g=-24.855\n",
      ">1, 792/896, d=-8.427, g=-32.101\n",
      ">1, 793/896, d=-4.068, g=26.588\n",
      ">1, 794/896, d=-8.294, g=37.561\n",
      ">1, 795/896, d=-4.327, g=28.071\n",
      ">1, 796/896, d=-7.902, g=33.994\n",
      ">1, 797/896, d=-10.199, g=45.012\n",
      ">1, 798/896, d=-8.814, g=53.620\n",
      ">1, 799/896, d=-10.957, g=38.299\n",
      ">1, 800/896, d=-2.301, g=27.349\n",
      ">1, 801/896, d=-1.182, g=26.469\n",
      ">1, 802/896, d=-6.187, g=29.751\n",
      ">1, 803/896, d=-7.092, g=27.225\n",
      ">1, 804/896, d=-7.061, g=31.258\n",
      ">1, 805/896, d=-8.498, g=46.890\n",
      ">1, 806/896, d=1.842, g=37.188\n",
      ">1, 807/896, d=-5.528, g=8.442\n",
      ">1, 808/896, d=-4.238, g=7.458\n",
      ">1, 809/896, d=-9.858, g=9.735\n",
      ">1, 810/896, d=-6.675, g=43.984\n",
      ">1, 811/896, d=-9.324, g=63.134\n",
      ">1, 812/896, d=-5.881, g=55.567\n",
      ">1, 813/896, d=-4.600, g=9.179\n",
      ">1, 814/896, d=-2.739, g=15.277\n",
      ">1, 815/896, d=-5.702, g=26.982\n",
      ">1, 816/896, d=-6.945, g=33.272\n",
      ">1, 817/896, d=-0.315, g=34.401\n",
      ">1, 818/896, d=-6.955, g=21.693\n",
      ">1, 819/896, d=-7.934, g=14.629\n",
      ">1, 820/896, d=-7.754, g=-9.431\n",
      ">1, 821/896, d=-4.952, g=-33.096\n",
      ">1, 822/896, d=-8.058, g=-38.967\n",
      ">1, 823/896, d=-15.533, g=-52.677\n",
      ">1, 824/896, d=-8.966, g=-61.721\n",
      ">1, 825/896, d=-10.175, g=-63.533\n",
      ">1, 826/896, d=-4.658, g=-39.440\n",
      ">1, 827/896, d=-9.469, g=29.456\n",
      ">1, 828/896, d=-0.875, g=40.887\n",
      ">1, 829/896, d=-6.869, g=27.831\n",
      ">1, 830/896, d=-6.081, g=30.275\n",
      ">1, 831/896, d=-5.620, g=29.337\n",
      ">1, 832/896, d=-5.773, g=26.044\n",
      ">1, 833/896, d=-1.608, g=17.921\n",
      ">1, 834/896, d=-9.135, g=7.463\n",
      ">1, 835/896, d=-9.079, g=4.527\n",
      ">1, 836/896, d=-5.618, g=4.215\n",
      ">1, 837/896, d=-7.096, g=12.234\n",
      ">1, 838/896, d=-7.394, g=20.056\n",
      ">1, 839/896, d=-6.851, g=5.415\n",
      ">1, 840/896, d=-9.372, g=7.723\n",
      ">1, 841/896, d=-7.490, g=6.837\n",
      ">1, 842/896, d=-5.125, g=8.363\n",
      ">1, 843/896, d=-10.874, g=2.802\n",
      ">1, 844/896, d=-4.638, g=9.463\n",
      ">1, 845/896, d=-3.611, g=18.174\n",
      ">1, 846/896, d=-9.085, g=22.070\n",
      ">1, 847/896, d=-10.325, g=17.642\n",
      ">1, 848/896, d=-6.024, g=24.799\n",
      ">1, 849/896, d=-5.693, g=18.754\n",
      ">1, 850/896, d=-8.076, g=26.095\n",
      ">1, 851/896, d=-5.800, g=10.696\n",
      ">1, 852/896, d=-3.194, g=10.138\n",
      ">1, 853/896, d=-8.254, g=8.942\n",
      ">1, 854/896, d=-10.239, g=2.072\n",
      ">1, 855/896, d=-7.745, g=-6.566\n",
      ">1, 856/896, d=-5.648, g=-12.306\n",
      ">1, 857/896, d=-6.978, g=-11.291\n",
      ">1, 858/896, d=-1.356, g=-14.174\n",
      ">1, 859/896, d=-4.210, g=-9.974\n",
      ">1, 860/896, d=-7.165, g=-7.622\n",
      ">1, 861/896, d=-2.516, g=-5.660\n",
      ">1, 862/896, d=-3.183, g=-3.865\n",
      ">1, 863/896, d=-6.981, g=-1.124\n",
      ">1, 864/896, d=-9.100, g=-4.674\n",
      ">1, 865/896, d=-7.609, g=-0.010\n",
      ">1, 866/896, d=-4.216, g=13.741\n",
      ">1, 867/896, d=-5.946, g=7.830\n",
      ">1, 868/896, d=-8.791, g=3.402\n",
      ">1, 869/896, d=-0.653, g=7.729\n",
      ">1, 870/896, d=-4.365, g=5.318\n",
      ">1, 871/896, d=-9.653, g=-2.194\n",
      ">1, 872/896, d=-6.003, g=-28.309\n",
      ">1, 873/896, d=-5.426, g=-50.458\n",
      ">1, 874/896, d=-5.751, g=-30.108\n",
      ">1, 875/896, d=-7.610, g=-25.217\n",
      ">1, 876/896, d=-7.542, g=-22.964\n",
      ">1, 877/896, d=-7.163, g=-24.502\n",
      ">1, 878/896, d=-8.465, g=-34.199\n",
      ">1, 879/896, d=-3.000, g=-20.330\n",
      ">1, 880/896, d=-8.649, g=-8.261\n",
      ">1, 881/896, d=-6.429, g=-0.253\n",
      ">1, 882/896, d=-6.426, g=36.696\n",
      ">1, 883/896, d=-7.857, g=43.394\n",
      ">1, 884/896, d=-9.504, g=18.827\n",
      ">1, 885/896, d=-8.553, g=9.118\n",
      ">1, 886/896, d=-5.044, g=1.976\n",
      ">1, 887/896, d=-6.133, g=-10.049\n",
      ">1, 888/896, d=-10.424, g=-23.924\n",
      ">1, 889/896, d=-4.704, g=-20.307\n",
      ">1, 890/896, d=-5.526, g=-20.486\n",
      ">1, 891/896, d=-6.613, g=-21.255\n",
      ">1, 892/896, d=-6.660, g=-17.685\n",
      ">1, 893/896, d=-7.219, g=-14.808\n",
      ">1, 894/896, d=-11.277, g=-21.926\n",
      ">1, 895/896, d=-2.586, g=-8.885\n",
      ">1, 896/896, d=-4.201, g=3.474\n",
      ">2, 1/896, d=-3.870, g=1.245\n",
      ">2, 2/896, d=-8.668, g=-5.577\n",
      ">2, 3/896, d=-7.969, g=-1.807\n",
      ">2, 4/896, d=-4.126, g=7.123\n",
      ">2, 5/896, d=-7.959, g=9.846\n",
      ">2, 6/896, d=-5.142, g=15.042\n",
      ">2, 7/896, d=-8.851, g=-1.596\n",
      ">2, 8/896, d=-4.279, g=-6.574\n",
      ">2, 9/896, d=-5.272, g=-4.918\n",
      ">2, 10/896, d=-2.156, g=2.412\n",
      ">2, 11/896, d=-3.135, g=-2.707\n",
      ">2, 12/896, d=-5.883, g=1.447\n",
      ">2, 13/896, d=-5.470, g=-2.144\n",
      ">2, 14/896, d=-5.971, g=-1.131\n",
      ">2, 15/896, d=-8.593, g=-3.147\n",
      ">2, 16/896, d=-5.205, g=-5.703\n",
      ">2, 17/896, d=-7.923, g=-1.869\n",
      ">2, 18/896, d=-5.872, g=16.627\n",
      ">2, 19/896, d=-4.076, g=4.033\n",
      ">2, 20/896, d=-8.688, g=-5.732\n",
      ">2, 21/896, d=-8.635, g=-1.161\n",
      ">2, 22/896, d=-9.348, g=-4.275\n",
      ">2, 23/896, d=-8.180, g=-6.632\n",
      ">2, 24/896, d=-6.506, g=-15.314\n",
      ">2, 25/896, d=-9.988, g=-1.847\n",
      ">2, 26/896, d=-8.412, g=-2.027\n",
      ">2, 27/896, d=-7.437, g=-5.062\n",
      ">2, 28/896, d=0.039, g=-11.117\n",
      ">2, 29/896, d=-8.947, g=-16.649\n",
      ">2, 30/896, d=-5.319, g=-19.928\n",
      ">2, 31/896, d=-4.589, g=-21.165\n",
      ">2, 32/896, d=-4.180, g=-31.099\n",
      ">2, 33/896, d=-8.628, g=-39.811\n",
      ">2, 34/896, d=-7.422, g=-28.879\n",
      ">2, 35/896, d=-4.201, g=-15.556\n",
      ">2, 36/896, d=-12.943, g=-13.156\n",
      ">2, 37/896, d=-8.781, g=-15.111\n",
      ">2, 38/896, d=-7.254, g=-30.547\n",
      ">2, 39/896, d=-2.802, g=-19.757\n",
      ">2, 40/896, d=-3.503, g=-21.080\n",
      ">2, 41/896, d=-6.636, g=-26.758\n",
      ">2, 42/896, d=-7.474, g=-28.438\n",
      ">2, 43/896, d=-4.930, g=-23.546\n",
      ">2, 44/896, d=-4.654, g=-12.389\n",
      ">2, 45/896, d=-3.805, g=8.843\n",
      ">2, 46/896, d=-5.912, g=29.632\n",
      ">2, 47/896, d=-7.241, g=21.916\n",
      ">2, 48/896, d=-6.374, g=25.961\n",
      ">2, 49/896, d=-6.736, g=30.605\n",
      ">2, 50/896, d=-8.356, g=29.111\n",
      ">2, 51/896, d=-5.738, g=35.297\n",
      ">2, 52/896, d=-3.322, g=37.246\n",
      ">2, 53/896, d=-8.542, g=33.376\n",
      ">2, 54/896, d=-5.234, g=16.882\n",
      ">2, 55/896, d=-6.761, g=20.064\n",
      ">2, 56/896, d=-7.564, g=2.593\n",
      ">2, 57/896, d=-0.532, g=-1.497\n",
      ">2, 58/896, d=-1.275, g=-0.776\n",
      ">2, 59/896, d=-10.444, g=-0.588\n",
      ">2, 60/896, d=-5.914, g=-6.974\n",
      ">2, 61/896, d=-6.455, g=-17.242\n",
      ">2, 62/896, d=-8.395, g=-33.969\n",
      ">2, 63/896, d=-8.870, g=-45.516\n",
      ">2, 64/896, d=-8.416, g=-50.399\n",
      ">2, 65/896, d=-5.419, g=-22.911\n",
      ">2, 66/896, d=-8.893, g=-23.762\n",
      ">2, 67/896, d=-3.506, g=-1.447\n",
      ">2, 68/896, d=-7.150, g=10.150\n",
      ">2, 69/896, d=-4.068, g=16.292\n",
      ">2, 70/896, d=-9.315, g=19.507\n",
      ">2, 71/896, d=-8.763, g=7.468\n",
      ">2, 72/896, d=-6.220, g=-3.367\n",
      ">2, 73/896, d=-9.189, g=-1.097\n",
      ">2, 74/896, d=-8.590, g=-2.877\n",
      ">2, 75/896, d=-5.941, g=-9.836\n",
      ">2, 76/896, d=-9.204, g=-18.514\n",
      ">2, 77/896, d=0.289, g=-13.144\n",
      ">2, 78/896, d=-3.997, g=3.144\n",
      ">2, 79/896, d=-4.702, g=-21.074\n",
      ">2, 80/896, d=-14.582, g=-31.770\n",
      ">2, 81/896, d=-5.436, g=21.419\n",
      ">2, 82/896, d=-8.621, g=30.833\n",
      ">2, 83/896, d=-6.282, g=31.699\n",
      ">2, 84/896, d=-6.269, g=31.287\n",
      ">2, 85/896, d=-9.902, g=37.872\n",
      ">2, 86/896, d=-7.551, g=26.616\n",
      ">2, 87/896, d=-2.209, g=12.447\n",
      ">2, 88/896, d=-4.998, g=5.849\n",
      ">2, 89/896, d=-6.930, g=7.476\n",
      ">2, 90/896, d=-2.921, g=9.199\n",
      ">2, 91/896, d=-5.343, g=5.716\n",
      ">2, 92/896, d=-2.957, g=7.762\n",
      ">2, 93/896, d=-4.054, g=1.042\n",
      ">2, 94/896, d=-6.414, g=1.279\n",
      ">2, 95/896, d=-12.499, g=-7.113\n",
      ">2, 96/896, d=-3.483, g=-14.889\n",
      ">2, 97/896, d=-5.447, g=-33.666\n",
      ">2, 98/896, d=-5.423, g=-37.213\n",
      ">2, 99/896, d=-13.149, g=-47.570\n",
      ">2, 100/896, d=-3.035, g=-47.450\n",
      ">2, 101/896, d=-9.652, g=-40.288\n",
      ">2, 102/896, d=-8.191, g=-31.809\n",
      ">2, 103/896, d=-8.956, g=-16.171\n",
      ">2, 104/896, d=-4.005, g=-9.131\n",
      ">2, 105/896, d=-7.988, g=-10.751\n",
      ">2, 106/896, d=-4.872, g=-11.037\n",
      ">2, 107/896, d=-8.244, g=-5.111\n",
      ">2, 108/896, d=-7.936, g=-5.084\n",
      ">2, 109/896, d=-8.572, g=-7.176\n",
      ">2, 110/896, d=-10.727, g=0.194\n",
      ">2, 111/896, d=-6.783, g=27.510\n",
      ">2, 112/896, d=-1.686, g=22.899\n",
      ">2, 113/896, d=-6.139, g=30.497\n",
      ">2, 114/896, d=-5.059, g=18.242\n",
      ">2, 115/896, d=-4.215, g=14.668\n",
      ">2, 116/896, d=-5.495, g=17.836\n",
      ">2, 117/896, d=-4.433, g=10.472\n",
      ">2, 118/896, d=-9.924, g=9.463\n",
      ">2, 119/896, d=-9.491, g=5.085\n",
      ">2, 120/896, d=-5.711, g=5.082\n",
      ">2, 121/896, d=-8.392, g=-1.899\n",
      ">2, 122/896, d=-6.452, g=-7.966\n",
      ">2, 123/896, d=-2.660, g=-7.973\n",
      ">2, 124/896, d=-4.137, g=-4.622\n",
      ">2, 125/896, d=-8.405, g=-4.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2, 126/896, d=-8.592, g=-14.299\n",
      ">2, 127/896, d=-1.286, g=-14.685\n",
      ">2, 128/896, d=-5.183, g=-14.207\n",
      ">2, 129/896, d=-4.963, g=-6.508\n",
      ">2, 130/896, d=-7.064, g=-3.602\n",
      ">2, 131/896, d=-7.833, g=-3.342\n",
      ">2, 132/896, d=-5.339, g=-6.239\n",
      ">2, 133/896, d=-12.867, g=-11.007\n",
      ">2, 134/896, d=-3.195, g=-10.691\n",
      ">2, 135/896, d=-8.836, g=-15.770\n",
      ">2, 136/896, d=-10.323, g=-9.791\n",
      ">2, 137/896, d=-6.621, g=-3.407\n",
      ">2, 138/896, d=-3.736, g=8.127\n",
      ">2, 139/896, d=-7.052, g=23.571\n",
      ">2, 140/896, d=-4.626, g=19.700\n",
      ">2, 141/896, d=-8.009, g=22.493\n",
      ">2, 142/896, d=-2.640, g=30.081\n",
      ">2, 143/896, d=-11.041, g=28.901\n",
      ">2, 144/896, d=-7.595, g=30.303\n",
      ">2, 145/896, d=-6.251, g=12.285\n",
      ">2, 146/896, d=-9.701, g=-7.824\n",
      ">2, 147/896, d=-8.933, g=-31.844\n",
      ">2, 148/896, d=-11.046, g=-52.888\n",
      ">2, 149/896, d=-5.844, g=-49.829\n",
      ">2, 150/896, d=-2.350, g=-36.704\n",
      ">2, 151/896, d=-7.009, g=-40.067\n",
      ">2, 152/896, d=-6.151, g=-31.048\n",
      ">2, 153/896, d=-3.188, g=-59.746\n",
      ">2, 154/896, d=-2.729, g=-47.457\n",
      ">2, 155/896, d=-9.527, g=-51.062\n",
      ">2, 156/896, d=-4.820, g=-33.333\n",
      ">2, 157/896, d=-5.205, g=-21.246\n",
      ">2, 158/896, d=-11.132, g=-12.239\n",
      ">2, 159/896, d=-7.398, g=-10.514\n",
      ">2, 160/896, d=-3.361, g=-5.162\n",
      ">2, 161/896, d=-6.455, g=-1.411\n",
      ">2, 162/896, d=-6.635, g=25.727\n",
      ">2, 163/896, d=-9.888, g=24.378\n",
      ">2, 164/896, d=-4.833, g=20.859\n",
      ">2, 165/896, d=-7.768, g=13.989\n",
      ">2, 166/896, d=-3.955, g=8.493\n",
      ">2, 167/896, d=-5.793, g=13.039\n",
      ">2, 168/896, d=-4.134, g=7.842\n",
      ">2, 169/896, d=-7.873, g=-5.612\n",
      ">2, 170/896, d=-6.893, g=-9.521\n",
      ">2, 171/896, d=-4.628, g=-11.597\n",
      ">2, 172/896, d=-3.453, g=-9.044\n",
      ">2, 173/896, d=-7.637, g=-18.476\n",
      ">2, 174/896, d=-5.788, g=-13.286\n",
      ">2, 175/896, d=-6.249, g=-16.567\n",
      ">2, 176/896, d=-1.908, g=-6.594\n",
      ">2, 177/896, d=-5.353, g=-17.688\n",
      ">2, 178/896, d=0.492, g=-1.868\n",
      ">2, 179/896, d=-5.091, g=2.045\n",
      ">2, 180/896, d=-4.817, g=8.305\n",
      ">2, 181/896, d=-8.019, g=13.224\n",
      ">2, 182/896, d=-2.654, g=-1.562\n",
      ">2, 183/896, d=-3.181, g=-7.587\n",
      ">2, 184/896, d=-4.828, g=-18.378\n",
      ">2, 185/896, d=-8.031, g=-50.193\n",
      ">2, 186/896, d=-0.575, g=-56.928\n",
      ">2, 187/896, d=-1.565, g=-43.378\n",
      ">2, 188/896, d=-4.741, g=-42.670\n",
      ">2, 189/896, d=-6.677, g=-36.768\n",
      ">2, 190/896, d=-9.188, g=-33.497\n",
      ">2, 191/896, d=-4.369, g=-33.589\n",
      ">2, 192/896, d=-8.790, g=-26.389\n",
      ">2, 193/896, d=-7.555, g=-8.481\n",
      ">2, 194/896, d=-7.373, g=0.217\n",
      ">2, 195/896, d=-10.443, g=-4.984\n",
      ">2, 196/896, d=-3.696, g=-2.658\n",
      ">2, 197/896, d=1.425, g=14.584\n",
      ">2, 198/896, d=-4.628, g=9.026\n",
      ">2, 199/896, d=-9.141, g=30.213\n",
      ">2, 200/896, d=-2.366, g=55.071\n",
      ">2, 201/896, d=-5.247, g=38.250\n",
      ">2, 202/896, d=-10.127, g=30.964\n",
      ">2, 203/896, d=-4.166, g=12.590\n",
      ">2, 204/896, d=-9.178, g=1.250\n",
      ">2, 205/896, d=-7.947, g=-0.807\n",
      ">2, 206/896, d=-9.624, g=4.650\n",
      ">2, 207/896, d=-1.823, g=2.416\n",
      ">2, 208/896, d=-9.361, g=4.841\n",
      ">2, 209/896, d=-7.100, g=2.833\n",
      ">2, 210/896, d=-7.923, g=4.600\n",
      ">2, 211/896, d=-4.472, g=11.700\n",
      ">2, 212/896, d=-1.436, g=13.517\n",
      ">2, 213/896, d=-5.518, g=10.062\n",
      ">2, 214/896, d=-6.914, g=4.647\n",
      ">2, 215/896, d=-6.670, g=6.939\n",
      ">2, 216/896, d=-5.079, g=9.211\n",
      ">2, 217/896, d=-6.745, g=9.994\n",
      ">2, 218/896, d=-4.384, g=11.405\n",
      ">2, 219/896, d=-3.390, g=15.492\n",
      ">2, 220/896, d=-10.076, g=27.952\n",
      ">2, 221/896, d=-7.011, g=21.727\n",
      ">2, 222/896, d=-7.273, g=19.196\n",
      ">2, 223/896, d=-6.693, g=16.333\n",
      ">2, 224/896, d=-7.699, g=11.588\n",
      ">2, 225/896, d=-0.988, g=6.186\n",
      ">2, 226/896, d=-8.226, g=9.568\n",
      ">2, 227/896, d=-1.002, g=2.643\n",
      ">2, 228/896, d=-7.912, g=2.848\n",
      ">2, 229/896, d=-1.593, g=5.830\n",
      ">2, 230/896, d=-1.680, g=0.778\n",
      ">2, 231/896, d=-3.232, g=-10.535\n",
      ">2, 232/896, d=-4.866, g=-31.769\n",
      ">2, 233/896, d=-4.079, g=-41.054\n",
      ">2, 234/896, d=-3.017, g=-31.009\n",
      ">2, 235/896, d=-3.401, g=-18.505\n",
      ">2, 236/896, d=1.531, g=-7.585\n",
      ">2, 237/896, d=-5.324, g=8.353\n",
      ">2, 238/896, d=-3.719, g=17.312\n",
      ">2, 239/896, d=-3.059, g=23.082\n",
      ">2, 240/896, d=-5.401, g=21.748\n",
      ">2, 241/896, d=-6.780, g=21.278\n",
      ">2, 242/896, d=-8.192, g=11.988\n",
      ">2, 243/896, d=-4.335, g=1.816\n",
      ">2, 244/896, d=-8.476, g=-6.431\n",
      ">2, 245/896, d=-3.037, g=-18.174\n",
      ">2, 246/896, d=-3.255, g=-29.212\n",
      ">2, 247/896, d=-9.372, g=-33.296\n",
      ">2, 248/896, d=-3.396, g=-55.073\n",
      ">2, 249/896, d=-8.263, g=-55.928\n",
      ">2, 250/896, d=-4.794, g=-43.733\n",
      ">2, 251/896, d=-8.597, g=-55.926\n",
      ">2, 252/896, d=-7.334, g=-27.309\n",
      ">2, 253/896, d=-7.804, g=-20.505\n",
      ">2, 254/896, d=-7.907, g=-16.350\n",
      ">2, 255/896, d=-4.386, g=7.156\n",
      ">2, 256/896, d=-2.078, g=0.049\n",
      ">2, 257/896, d=-4.923, g=14.231\n",
      ">2, 258/896, d=-1.069, g=6.423\n",
      ">2, 259/896, d=-2.342, g=0.782\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    iteration = train_size // BATCH_SIZE\n",
    "    for i in range(iteration):\n",
    "        real_images = list(dataset.take(1))[0]\n",
    "        data_losses = wgan.train_step(real_images)\n",
    "        print('>%d, %d/%d, d=%.3f, g=%.3f' %\n",
    "            (ep+1, i+1, iteration, data_losses['d_loss'], data_losses['g_loss']))\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            cbk.on_epoch_end(f'i_{i}_ep_{ep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.save_weights('wgan_generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65909b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
