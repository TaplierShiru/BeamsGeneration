{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7708e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/generative/wgan_gp/\n",
    "#\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "\n",
    "# example of loading the generator model and generating images\n",
    "from numpy import asarray\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4e7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "  images = (images - 127.5) / 127.5\n",
    "  return images.astype('float32')\n",
    "\n",
    "def generator_img(data_list: list):\n",
    "    counter = 0\n",
    "    max_counter = len(data_list)\n",
    "    while True:\n",
    "        image_s, label_s = data_list[counter]\n",
    "        image_s = preprocess_images(np.asarray(image_s, dtype=np.float32))\n",
    "        yield np.expand_dims(image_s, axis=-1), label_s\n",
    "        counter += 1\n",
    "\n",
    "        if counter == max_counter:\n",
    "            counter = 0\n",
    "            data_list = shuffle(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90fd9501",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (28, 28, 1)\n",
    "BATCH_SIZE = 16\n",
    "N_CLASSES = 10\n",
    "# Size of the noise vector\n",
    "noise_dim = 100\n",
    "\n",
    "SAVE_RESULT = 'exp_result_new_ideas'\n",
    "\n",
    "train_images_path = []\n",
    "\n",
    "(x_train, y_train), (_, _) = load_data()\n",
    "\n",
    "data_list = list(zip(x_train, y_train))\n",
    "data_list = shuffle(data_list)\n",
    "\n",
    "def train_gen():\n",
    "    return generator_img(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff0051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_generator(\n",
    "        train_gen, \n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=IMG_SHAPE, dtype=np.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=np.int32),\n",
    "        )\n",
    "    )\n",
    "    .shuffle(BATCH_SIZE * 500).batch(BATCH_SIZE).prefetch(6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ad97db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 60000\n"
     ]
    }
   ],
   "source": [
    "train_size = len(data_list)\n",
    "\n",
    "print(f'train: {train_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c60a3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights():\n",
    "    return initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "def init_weights():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671ad94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNInferenceMode(tf.Module):\n",
    "    def __init__(self, dim, eps=1e-3):\n",
    "        val = np.ones(dim, dtype='float32')\n",
    "        self.gamma = tf.Variable(val, name='BN/gamma')\n",
    "        val = np.zeros(dim, dtype='float32')\n",
    "        self.beta = tf.Variable(val, name='BN/beta')\n",
    "        self.eps = eps\n",
    "    \n",
    "    def __call__(self, x, training=False):\n",
    "        mean, var = tf.nn.moments(x, axes=[0, 1, 2], keepdims=True)\n",
    "        return tf.nn.batch_normalization(\n",
    "            x=x,\n",
    "            mean=mean,\n",
    "            variance=var,\n",
    "            offset=self.beta,\n",
    "            scale=self.gamma,\n",
    "            variance_epsilon=self.eps,\n",
    "            name='CustomBN'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af24aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorModel(tf.Module):\n",
    "    def __init__(self, out_dim, n_classes=10, h_low=3, w_low=3):\n",
    "        super().__init__()\n",
    "        # Labels inputs\n",
    "        self.label_layers_l = self._init_label_input_branch(n_classes, h_low, w_low)\n",
    "        # Noise inputs\n",
    "        self.noise_layers_l = self._init_noise_input_branch(h_low, w_low)\n",
    "        # Merge layer (concat)\n",
    "        self.merge = Concatenate()\n",
    "        # Model layers \n",
    "        self.model_layers_l = self._init_model_branch(out_dim)\n",
    "        # Final layer\n",
    "        self.final_layer = self.model_layers_l[-1]\n",
    "\n",
    "    def _init_label_input_branch(self, n_classes, h_low, w_low):\n",
    "        n_nodes = h_low * w_low\n",
    "        label_layers_l = [\n",
    "            Embedding(n_classes, 64),\n",
    "            Dense(n_nodes),\n",
    "            Reshape((h_low, w_low, 1))\n",
    "        ]\n",
    "        return label_layers_l\n",
    "\n",
    "    def _init_noise_input_branch(self, h_low, w_low):\n",
    "        # foundation for h_low x w_low image\n",
    "        n_nodes = 32 * h_low * w_low\n",
    "\n",
    "        noise_layers_l = [\n",
    "            Dense(n_nodes),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "            Reshape((h_low, w_low, 32))\n",
    "        ]\n",
    "        return noise_layers_l\n",
    "\n",
    "    def _init_model_branch(self, out_dim):\n",
    "        model_layers_l = [\n",
    "            Conv2DTranspose(\n",
    "                128, (3,3), \n",
    "                strides=(2,2), padding='same',\n",
    "                kernel_initializer=init_weights()\n",
    "            ), # 6\n",
    "            L.ZeroPadding2D(((1, 0), (1, 0))), # 7\n",
    "            #BNInferenceMode(128),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "            \n",
    "            Conv2DTranspose(\n",
    "                64, (3,3), \n",
    "                strides=(2,2), padding='same',\n",
    "                kernel_initializer=init_weights()\n",
    "            ), # 14\n",
    "            #BNInferenceMode(128),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "\n",
    "            Conv2DTranspose(\n",
    "                out_dim, (3,3), activation='tanh', \n",
    "                strides=(2,2), padding='same',\n",
    "                kernel_initializer=init_weights()\n",
    "            ), # 28\n",
    "\n",
    "        ]\n",
    "        return model_layers_l\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, label_i, noise_i, training=False):\n",
    "        # Label branch\n",
    "        for layer_label_i in self.label_layers_l:\n",
    "            label_i = layer_label_i(label_i, training=training)\n",
    "\n",
    "        # Noise branch\n",
    "        for layer_noise_i in self.noise_layers_l:\n",
    "            noise_i = layer_noise_i(noise_i, training=training)\n",
    "\n",
    "        x_t = self.merge([noise_i, label_i])\n",
    "        # Model branch\n",
    "        for layer_model_i in self.model_layers_l:\n",
    "            x_t = layer_model_i(x_t, training=training)\n",
    "        \n",
    "        return x_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddef0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscModel(tf.Module):\n",
    "    def __init__(self, in_shape, out_dim, n_classes=10):\n",
    "        super().__init__()\n",
    "        # Labels inputs\n",
    "        self.label_layers_l = self._init_label_input_branch(in_shape, n_classes)\n",
    "        # Noise inputs\n",
    "        self.image_layers_l = self._init_image_input_branch()\n",
    "        # Merge layer (concat)\n",
    "        self.merge = Concatenate()\n",
    "        # Model layers \n",
    "        self.model_layers_l = self._init_model_branch(out_dim)\n",
    "        # Final layer\n",
    "        self.final_layer = self.model_layers_l[-1]\n",
    "\n",
    "    def _init_label_input_branch(self, in_shape, n_classes):\n",
    "        n_nodes = in_shape[0] * in_shape[1] * in_shape[2]\n",
    "        label_layers_l = [\n",
    "            # embedding for categorical input\n",
    "            Embedding(n_classes, 128),\n",
    "            # scale up to image dimensions with linear activation\n",
    "            Dense(n_nodes),\n",
    "            Reshape((in_shape[0], in_shape[1], in_shape[2]))\n",
    "        ]\n",
    "        return label_layers_l\n",
    "\n",
    "    def _init_image_input_branch(self):\n",
    "        image_layers_l = []\n",
    "        return image_layers_l\n",
    "\n",
    "    def _init_model_branch(self, out_dim):\n",
    "        model_layers_l = [\n",
    "            Conv2D(\n",
    "                64, (3,3), \n",
    "                strides=(2,2), padding='same', \n",
    "                kernel_initializer=init_weights()\n",
    "            ), # 14\n",
    "            #BatchNormalization(),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "\n",
    "            Conv2D(\n",
    "                128, (3,3), \n",
    "                strides=(2,2), padding='same', \n",
    "                kernel_initializer=init_weights()\n",
    "            ), # 7  \n",
    "            #BatchNormalization(),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "\n",
    "            Conv2D(\n",
    "                64, (3,3), \n",
    "                strides=(2,2), padding='same', \n",
    "                kernel_initializer=init_weights()\n",
    "            ), # 3  \n",
    "            #BatchNormalization(),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "            \n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(out_dim),\n",
    "        ]\n",
    "        return model_layers_l\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, label_i, image_i, training=False):\n",
    "        # Label branch\n",
    "        for layer_label_i in self.label_layers_l:\n",
    "            label_i = layer_label_i(label_i, training=training)\n",
    "        \n",
    "        # Image branch\n",
    "        for layer_image_i in self.image_layers_l:\n",
    "            image_i = layer_image_i(image_i, training=training)\n",
    "        \n",
    "        x_t = self.merge([image_i, label_i])\n",
    "        # Model branch\n",
    "        for layer_model_i in self.model_layers_l:\n",
    "            x_t = layer_model_i(x_t, training=training)\n",
    "        return x_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68bd388a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_model = DiscModel(in_shape=IMG_SHAPE, out_dim=1, n_classes=N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e022ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_model = GeneratorModel(IMG_SHAPE[-1], n_classes=N_CLASSES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q = g_model.model_layers_l[0]\n",
    "#q.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c1c33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    @tf.function\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images, real_labels):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(label_i=real_labels, image_i=interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated, real_labels])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images, real_labels):\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            d_loss = self._disc_train_step(real_images, real_labels)\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        g_loss = self._generator_train_step(batch_size)\n",
    "\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "    @tf.function\n",
    "    def _generator_train_step(self, batch_size):\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_labels = tf.random.uniform([batch_size], minval=0, maxval=N_CLASSES, dtype=tf.int32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator(label_i=random_labels, noise_i=random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(label_i=random_labels, image_i=generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return g_loss\n",
    "\n",
    "    @tf.function\n",
    "    def _disc_train_step(self, real_images, real_labels):\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(batch_size, self.latent_dim)\n",
    "        )\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images from the latent vector\n",
    "            fake_images = self.generator(label_i=real_labels, noise_i=random_latent_vectors, training=True)\n",
    "            # Get the logits for the fake images\n",
    "            fake_logits = self.discriminator(label_i=real_labels, image_i=fake_images, training=True)\n",
    "            # Get the logits for the real images\n",
    "            real_logits = self.discriminator(label_i=real_labels, image_i=real_images, training=True)\n",
    "\n",
    "            # Calculate the discriminator loss using the fake and real image logits\n",
    "            d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "            # Calculate the gradient penalty\n",
    "            gp = self.gradient_penalty(batch_size, real_images, fake_images, real_labels)\n",
    "            # Add the gradient penalty to the original discriminator loss\n",
    "            d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "        # Get the gradients w.r.t the discriminator loss\n",
    "        d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        # Update the weights of the discriminator using the discriminator optimizer\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(d_gradient, self.discriminator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return d_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2b39743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor():\n",
    "    def __init__(self, model, num_img=100, latent_dim=128, call_sign=None):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = model\n",
    "        self.call_sign = call_sign\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None, save_path=''):\n",
    "        n = int(np.sqrt(self.num_img))\n",
    "        random_latent_vectors = np.random.normal(size=(self.num_img, self.latent_dim))\n",
    "        random_labels = np.asarray([min(x, N_CLASSES-1)  for _ in range(10) for x in range(10)])\n",
    "        generated_images = self.model(label_i=random_labels, noise_i=random_latent_vectors)\n",
    "        # scale from [-1,1] to [0,1]\n",
    "        generated_images = (generated_images + 1) / 2.0\n",
    "        self._generate_plot(generated_images, n, os.path.join(save_path, f'{epoch}'))\n",
    "    \n",
    "    def _generate_plot(self, examples, n, prefix):\n",
    "        # plot images\n",
    "        fig = plt.figure(figsize=(12,12))\n",
    "        for i in range(n * n):\n",
    "            # define subplot\n",
    "            plt.subplot(n, n, 1 + i)\n",
    "            # turn off axis\n",
    "            plt.axis('off')\n",
    "            # plot raw pixel data\n",
    "            plt.imshow(examples[i])\n",
    "        #pyplot.show()\n",
    "        fig.savefig(f'{prefix}_image.png')\n",
    "        plt.close('all')\n",
    "    \n",
    "    def save_model(self, path_save):\n",
    "        if self.call_sign is None:\n",
    "            raise ValueError(\"call sign is not set in monitor class\")\n",
    "        \n",
    "        tf.saved_model.save(self.model, path_save, signatures=self.call_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2322272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "class GCClearCallback:\n",
    "    def on_epoch_end(self, epoch=0, logs=None):\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7373ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "# Set the number of epochs for trainining.\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "# Stuf in order to save model\n",
    "call = g_model.__call__.get_concrete_function(\n",
    "    tf.TensorSpec((1, 1), tf.int32, name='label'), tf.TensorSpec((1, noise_dim), tf.float32, name='noise')\n",
    ")\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "cbk = GANMonitor(\n",
    "    g_model,\n",
    "    num_img=100, latent_dim=noise_dim,\n",
    "    call_sign=call\n",
    ")\n",
    "gcclear_call = GCClearCallback\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=5, # was 3\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0937e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      ">1, 1/3750, d=7.813, g=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_0_ep_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_0_ep_0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 2/3750, d=6.775, g=0.003\n",
      ">1, 3/3750, d=5.394, g=-0.007\n",
      ">1, 4/3750, d=3.205, g=-0.034\n",
      ">1, 5/3750, d=0.328, g=-0.065\n",
      ">1, 6/3750, d=-2.924, g=-0.136\n",
      ">1, 7/3750, d=-7.841, g=-0.328\n",
      ">1, 8/3750, d=-12.535, g=-0.539\n",
      ">1, 9/3750, d=-13.395, g=-0.821\n",
      ">1, 10/3750, d=-15.553, g=-1.151\n",
      ">1, 11/3750, d=-17.017, g=-1.390\n",
      ">1, 12/3750, d=-15.151, g=-1.625\n",
      ">1, 13/3750, d=-21.343, g=-2.072\n",
      ">1, 14/3750, d=-21.636, g=-2.634\n",
      ">1, 15/3750, d=-25.020, g=-2.898\n",
      ">1, 16/3750, d=-23.733, g=-2.804\n",
      ">1, 17/3750, d=-25.094, g=-3.197\n",
      ">1, 18/3750, d=-27.494, g=-3.731\n",
      ">1, 19/3750, d=-24.510, g=-3.825\n",
      ">1, 20/3750, d=-26.575, g=-3.642\n",
      ">1, 21/3750, d=-29.549, g=-4.132\n",
      ">1, 22/3750, d=-29.384, g=-3.525\n",
      ">1, 23/3750, d=-30.444, g=-3.883\n",
      ">1, 24/3750, d=-30.001, g=-4.021\n",
      ">1, 25/3750, d=-29.872, g=-4.486\n",
      ">1, 26/3750, d=-31.421, g=-4.166\n",
      ">1, 27/3750, d=-30.277, g=-3.145\n",
      ">1, 28/3750, d=-29.273, g=-3.704\n",
      ">1, 29/3750, d=-28.736, g=-3.268\n",
      ">1, 30/3750, d=-29.713, g=-3.766\n",
      ">1, 31/3750, d=-31.558, g=-3.639\n",
      ">1, 32/3750, d=-31.017, g=-2.294\n",
      ">1, 33/3750, d=-29.272, g=-2.396\n",
      ">1, 34/3750, d=-33.291, g=-1.659\n",
      ">1, 35/3750, d=-33.311, g=-1.555\n",
      ">1, 36/3750, d=-32.975, g=-2.182\n",
      ">1, 37/3750, d=-32.543, g=-1.182\n",
      ">1, 38/3750, d=-30.706, g=-1.983\n",
      ">1, 39/3750, d=-35.790, g=-1.556\n",
      ">1, 40/3750, d=-34.263, g=-1.008\n",
      ">1, 41/3750, d=-36.953, g=-1.559\n",
      ">1, 42/3750, d=-33.998, g=-1.019\n",
      ">1, 43/3750, d=-31.365, g=-0.491\n",
      ">1, 44/3750, d=-32.094, g=-0.708\n",
      ">1, 45/3750, d=-33.756, g=-0.828\n",
      ">1, 46/3750, d=-34.280, g=-0.535\n",
      ">1, 47/3750, d=-33.883, g=0.643\n",
      ">1, 48/3750, d=-33.639, g=0.854\n",
      ">1, 49/3750, d=-34.704, g=1.344\n",
      ">1, 50/3750, d=-34.691, g=2.356\n",
      ">1, 51/3750, d=-38.513, g=2.206\n",
      ">1, 52/3750, d=-35.584, g=3.192\n",
      ">1, 53/3750, d=-34.137, g=3.903\n",
      ">1, 54/3750, d=-39.964, g=4.658\n",
      ">1, 55/3750, d=-34.427, g=4.932\n",
      ">1, 56/3750, d=-35.360, g=5.855\n",
      ">1, 57/3750, d=-34.067, g=6.794\n",
      ">1, 58/3750, d=-36.157, g=7.518\n",
      ">1, 59/3750, d=-37.241, g=8.481\n",
      ">1, 60/3750, d=-32.237, g=9.416\n",
      ">1, 61/3750, d=-33.893, g=9.508\n",
      ">1, 62/3750, d=-34.511, g=10.840\n",
      ">1, 63/3750, d=-37.153, g=10.889\n",
      ">1, 64/3750, d=-39.208, g=11.926\n",
      ">1, 65/3750, d=-35.802, g=12.784\n",
      ">1, 66/3750, d=-37.546, g=12.925\n",
      ">1, 67/3750, d=-35.300, g=14.181\n",
      ">1, 68/3750, d=-31.299, g=14.502\n",
      ">1, 69/3750, d=-35.144, g=15.462\n",
      ">1, 70/3750, d=-35.554, g=16.683\n",
      ">1, 71/3750, d=-40.325, g=17.132\n",
      ">1, 72/3750, d=-35.606, g=18.045\n",
      ">1, 73/3750, d=-37.263, g=19.331\n",
      ">1, 74/3750, d=-34.782, g=20.713\n",
      ">1, 75/3750, d=-32.767, g=21.776\n",
      ">1, 76/3750, d=-36.301, g=21.389\n",
      ">1, 77/3750, d=-32.934, g=24.285\n",
      ">1, 78/3750, d=-38.330, g=23.989\n",
      ">1, 79/3750, d=-34.453, g=25.271\n",
      ">1, 80/3750, d=-38.547, g=26.858\n",
      ">1, 81/3750, d=-37.926, g=27.712\n",
      ">1, 82/3750, d=-33.605, g=30.282\n",
      ">1, 83/3750, d=-38.276, g=29.602\n",
      ">1, 84/3750, d=-32.047, g=32.188\n",
      ">1, 85/3750, d=-38.609, g=34.353\n",
      ">1, 86/3750, d=-38.408, g=36.218\n",
      ">1, 87/3750, d=-32.885, g=36.941\n",
      ">1, 88/3750, d=-36.799, g=39.654\n",
      ">1, 89/3750, d=-42.077, g=40.610\n",
      ">1, 90/3750, d=-34.016, g=43.125\n",
      ">1, 91/3750, d=-35.364, g=42.603\n",
      ">1, 92/3750, d=-33.167, g=43.961\n",
      ">1, 93/3750, d=-36.070, g=44.297\n",
      ">1, 94/3750, d=-41.644, g=47.732\n",
      ">1, 95/3750, d=-38.836, g=49.342\n",
      ">1, 96/3750, d=-42.444, g=48.000\n",
      ">1, 97/3750, d=-32.878, g=53.669\n",
      ">1, 98/3750, d=-35.339, g=51.533\n",
      ">1, 99/3750, d=-29.992, g=51.920\n",
      ">1, 100/3750, d=-36.209, g=53.968\n",
      ">1, 101/3750, d=-39.225, g=53.525\n",
      ">1, 102/3750, d=-42.227, g=53.515\n",
      ">1, 103/3750, d=-38.140, g=57.314\n",
      ">1, 104/3750, d=-39.141, g=56.669\n",
      ">1, 105/3750, d=-32.219, g=59.542\n",
      ">1, 106/3750, d=-40.735, g=57.631\n",
      ">1, 107/3750, d=-38.243, g=60.937\n",
      ">1, 108/3750, d=-34.182, g=64.632\n",
      ">1, 109/3750, d=-35.762, g=62.646\n",
      ">1, 110/3750, d=-43.125, g=66.108\n",
      ">1, 111/3750, d=-32.124, g=69.764\n",
      ">1, 112/3750, d=-35.825, g=68.734\n",
      ">1, 113/3750, d=-37.137, g=71.859\n",
      ">1, 114/3750, d=-38.303, g=73.935\n",
      ">1, 115/3750, d=-33.407, g=70.914\n",
      ">1, 116/3750, d=-37.311, g=75.908\n",
      ">1, 117/3750, d=-36.075, g=75.856\n",
      ">1, 118/3750, d=-31.771, g=75.611\n",
      ">1, 119/3750, d=-31.602, g=75.915\n",
      ">1, 120/3750, d=-29.876, g=73.874\n",
      ">1, 121/3750, d=-36.023, g=72.806\n",
      ">1, 122/3750, d=-32.862, g=75.479\n",
      ">1, 123/3750, d=-35.356, g=74.719\n",
      ">1, 124/3750, d=-35.266, g=72.706\n",
      ">1, 125/3750, d=-32.581, g=80.122\n",
      ">1, 126/3750, d=-36.605, g=84.768\n",
      ">1, 127/3750, d=-37.054, g=82.660\n",
      ">1, 128/3750, d=-42.805, g=85.083\n",
      ">1, 129/3750, d=-32.810, g=84.726\n",
      ">1, 130/3750, d=-36.445, g=79.974\n",
      ">1, 131/3750, d=-37.978, g=86.143\n",
      ">1, 132/3750, d=-30.376, g=86.427\n",
      ">1, 133/3750, d=-39.863, g=86.664\n",
      ">1, 134/3750, d=-38.972, g=85.644\n",
      ">1, 135/3750, d=-41.801, g=86.986\n",
      ">1, 136/3750, d=-37.748, g=93.272\n",
      ">1, 137/3750, d=-37.603, g=91.227\n",
      ">1, 138/3750, d=-36.845, g=89.456\n",
      ">1, 139/3750, d=-37.236, g=88.919\n",
      ">1, 140/3750, d=-41.072, g=90.257\n",
      ">1, 141/3750, d=-41.123, g=90.035\n",
      ">1, 142/3750, d=-35.231, g=89.681\n",
      ">1, 143/3750, d=-30.366, g=93.343\n",
      ">1, 144/3750, d=-35.980, g=96.595\n",
      ">1, 145/3750, d=-31.201, g=96.737\n",
      ">1, 146/3750, d=-31.690, g=93.682\n",
      ">1, 147/3750, d=-30.621, g=93.299\n",
      ">1, 148/3750, d=-33.362, g=98.897\n",
      ">1, 149/3750, d=-29.413, g=97.416\n",
      ">1, 150/3750, d=-29.051, g=103.337\n",
      ">1, 151/3750, d=-34.122, g=105.477\n",
      ">1, 152/3750, d=-30.120, g=107.679\n",
      ">1, 153/3750, d=-34.970, g=103.122\n",
      ">1, 154/3750, d=-29.877, g=99.843\n",
      ">1, 155/3750, d=-34.959, g=104.198\n",
      ">1, 156/3750, d=-32.018, g=105.950\n",
      ">1, 157/3750, d=-34.405, g=107.761\n",
      ">1, 158/3750, d=-28.680, g=104.499\n",
      ">1, 159/3750, d=-28.260, g=108.159\n",
      ">1, 160/3750, d=-29.055, g=110.368\n",
      ">1, 161/3750, d=-43.143, g=97.849\n",
      ">1, 162/3750, d=-33.701, g=111.047\n",
      ">1, 163/3750, d=-33.511, g=109.305\n",
      ">1, 164/3750, d=-34.855, g=106.304\n",
      ">1, 165/3750, d=-30.551, g=106.706\n",
      ">1, 166/3750, d=-29.050, g=105.746\n",
      ">1, 167/3750, d=-24.497, g=98.735\n",
      ">1, 168/3750, d=-21.342, g=106.287\n",
      ">1, 169/3750, d=-33.579, g=101.727\n",
      ">1, 170/3750, d=-34.255, g=102.433\n",
      ">1, 171/3750, d=-37.196, g=106.604\n",
      ">1, 172/3750, d=-34.473, g=108.358\n",
      ">1, 173/3750, d=-42.199, g=107.826\n",
      ">1, 174/3750, d=-31.343, g=109.346\n",
      ">1, 175/3750, d=-38.614, g=106.038\n",
      ">1, 176/3750, d=-32.817, g=105.520\n",
      ">1, 177/3750, d=-36.330, g=107.974\n",
      ">1, 178/3750, d=-30.269, g=106.233\n",
      ">1, 179/3750, d=-30.043, g=103.454\n",
      ">1, 180/3750, d=-25.011, g=105.156\n",
      ">1, 181/3750, d=-23.186, g=115.546\n",
      ">1, 182/3750, d=-32.751, g=105.815\n",
      ">1, 183/3750, d=-20.324, g=106.131\n",
      ">1, 184/3750, d=-37.154, g=111.776\n",
      ">1, 185/3750, d=-34.228, g=105.762\n",
      ">1, 186/3750, d=-32.793, g=111.246\n",
      ">1, 187/3750, d=-26.698, g=107.415\n",
      ">1, 188/3750, d=-32.707, g=107.244\n",
      ">1, 189/3750, d=-34.192, g=109.642\n",
      ">1, 190/3750, d=-22.902, g=111.219\n",
      ">1, 191/3750, d=-21.018, g=106.625\n",
      ">1, 192/3750, d=-34.298, g=105.483\n",
      ">1, 193/3750, d=-24.774, g=100.173\n",
      ">1, 194/3750, d=-37.172, g=114.011\n",
      ">1, 195/3750, d=-31.157, g=113.811\n",
      ">1, 196/3750, d=-24.397, g=113.657\n",
      ">1, 197/3750, d=-29.256, g=104.939\n",
      ">1, 198/3750, d=-24.487, g=110.505\n",
      ">1, 199/3750, d=-34.623, g=103.521\n",
      ">1, 200/3750, d=-22.417, g=109.844\n",
      ">1, 201/3750, d=-31.694, g=109.068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_200_ep_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_200_ep_0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 202/3750, d=-30.351, g=112.799\n",
      ">1, 203/3750, d=-32.232, g=118.705\n",
      ">1, 204/3750, d=-30.494, g=107.501\n",
      ">1, 205/3750, d=-30.431, g=120.018\n",
      ">1, 206/3750, d=-34.128, g=120.777\n",
      ">1, 207/3750, d=-30.972, g=114.540\n",
      ">1, 208/3750, d=-30.983, g=106.233\n",
      ">1, 209/3750, d=-27.313, g=111.056\n",
      ">1, 210/3750, d=-26.391, g=112.737\n",
      ">1, 211/3750, d=-27.458, g=108.384\n",
      ">1, 212/3750, d=-32.368, g=102.175\n",
      ">1, 213/3750, d=-32.573, g=114.011\n",
      ">1, 214/3750, d=-27.917, g=109.485\n",
      ">1, 215/3750, d=-23.052, g=112.086\n",
      ">1, 216/3750, d=-35.846, g=117.105\n",
      ">1, 217/3750, d=-27.430, g=114.205\n",
      ">1, 218/3750, d=-31.443, g=111.507\n",
      ">1, 219/3750, d=-27.614, g=104.951\n",
      ">1, 220/3750, d=-26.973, g=100.300\n",
      ">1, 221/3750, d=-29.244, g=98.642\n",
      ">1, 222/3750, d=-31.972, g=100.367\n",
      ">1, 223/3750, d=-30.388, g=106.430\n",
      ">1, 224/3750, d=-22.131, g=108.269\n",
      ">1, 225/3750, d=-16.046, g=107.398\n",
      ">1, 226/3750, d=-27.484, g=97.857\n",
      ">1, 227/3750, d=-31.919, g=101.141\n",
      ">1, 228/3750, d=-30.768, g=100.143\n",
      ">1, 229/3750, d=-29.720, g=102.739\n",
      ">1, 230/3750, d=-27.453, g=102.712\n",
      ">1, 231/3750, d=-26.374, g=105.321\n",
      ">1, 232/3750, d=-25.665, g=100.694\n",
      ">1, 233/3750, d=-23.024, g=97.849\n",
      ">1, 234/3750, d=-28.800, g=100.159\n",
      ">1, 235/3750, d=-35.282, g=95.991\n",
      ">1, 236/3750, d=-25.537, g=100.868\n",
      ">1, 237/3750, d=-28.366, g=108.635\n",
      ">1, 238/3750, d=-29.509, g=106.700\n",
      ">1, 239/3750, d=-23.263, g=96.251\n",
      ">1, 240/3750, d=-34.437, g=105.985\n",
      ">1, 241/3750, d=-26.971, g=108.926\n",
      ">1, 242/3750, d=-24.236, g=110.633\n",
      ">1, 243/3750, d=-27.961, g=111.648\n",
      ">1, 244/3750, d=-34.792, g=104.533\n",
      ">1, 245/3750, d=-22.470, g=96.679\n",
      ">1, 246/3750, d=-28.688, g=97.323\n",
      ">1, 247/3750, d=-26.114, g=109.512\n",
      ">1, 248/3750, d=-25.010, g=108.962\n",
      ">1, 249/3750, d=-29.190, g=109.908\n",
      ">1, 250/3750, d=-24.925, g=103.892\n",
      ">1, 251/3750, d=-20.896, g=107.597\n",
      ">1, 252/3750, d=-26.167, g=102.691\n",
      ">1, 253/3750, d=-27.773, g=99.444\n",
      ">1, 254/3750, d=-23.364, g=100.245\n",
      ">1, 255/3750, d=-22.934, g=105.466\n",
      ">1, 256/3750, d=-26.709, g=99.315\n",
      ">1, 257/3750, d=-29.138, g=96.347\n",
      ">1, 258/3750, d=-25.317, g=94.561\n",
      ">1, 259/3750, d=-27.354, g=99.586\n",
      ">1, 260/3750, d=-22.721, g=98.893\n",
      ">1, 261/3750, d=-28.814, g=90.364\n",
      ">1, 262/3750, d=-20.580, g=93.804\n",
      ">1, 263/3750, d=-24.015, g=103.220\n",
      ">1, 264/3750, d=-27.734, g=87.598\n",
      ">1, 265/3750, d=-20.301, g=74.664\n",
      ">1, 266/3750, d=-21.457, g=92.664\n",
      ">1, 267/3750, d=-27.566, g=87.086\n",
      ">1, 268/3750, d=-16.968, g=82.846\n",
      ">1, 269/3750, d=-25.838, g=82.060\n",
      ">1, 270/3750, d=-24.934, g=90.676\n",
      ">1, 271/3750, d=-28.736, g=86.900\n",
      ">1, 272/3750, d=-25.593, g=82.058\n",
      ">1, 273/3750, d=-26.223, g=78.189\n",
      ">1, 274/3750, d=-16.812, g=75.504\n",
      ">1, 275/3750, d=-33.903, g=83.154\n",
      ">1, 276/3750, d=-30.626, g=74.360\n",
      ">1, 277/3750, d=-28.533, g=79.311\n",
      ">1, 278/3750, d=-27.318, g=79.984\n",
      ">1, 279/3750, d=-20.070, g=72.015\n",
      ">1, 280/3750, d=-21.239, g=80.873\n",
      ">1, 281/3750, d=-24.678, g=77.948\n",
      ">1, 282/3750, d=-28.166, g=73.760\n",
      ">1, 283/3750, d=-23.184, g=70.027\n",
      ">1, 284/3750, d=-27.223, g=69.916\n",
      ">1, 285/3750, d=-29.735, g=81.376\n",
      ">1, 286/3750, d=-18.436, g=78.521\n",
      ">1, 287/3750, d=-18.489, g=81.264\n",
      ">1, 288/3750, d=-28.641, g=74.364\n",
      ">1, 289/3750, d=-28.774, g=86.410\n",
      ">1, 290/3750, d=-30.692, g=81.855\n",
      ">1, 291/3750, d=-25.510, g=80.011\n",
      ">1, 292/3750, d=-23.871, g=81.888\n",
      ">1, 293/3750, d=-19.000, g=90.690\n",
      ">1, 294/3750, d=-26.229, g=82.465\n",
      ">1, 295/3750, d=-29.316, g=85.070\n",
      ">1, 296/3750, d=-22.853, g=69.302\n",
      ">1, 297/3750, d=-21.478, g=80.362\n",
      ">1, 298/3750, d=-28.638, g=78.043\n",
      ">1, 299/3750, d=-23.296, g=78.155\n",
      ">1, 300/3750, d=-22.904, g=81.298\n",
      ">1, 301/3750, d=-24.772, g=69.714\n",
      ">1, 302/3750, d=-21.391, g=77.248\n",
      ">1, 303/3750, d=-21.241, g=74.511\n",
      ">1, 304/3750, d=-22.597, g=70.059\n",
      ">1, 305/3750, d=-19.193, g=71.726\n",
      ">1, 306/3750, d=-21.477, g=77.365\n",
      ">1, 307/3750, d=-22.682, g=69.470\n",
      ">1, 308/3750, d=-31.103, g=74.064\n",
      ">1, 309/3750, d=-29.841, g=73.633\n",
      ">1, 310/3750, d=-24.895, g=73.766\n",
      ">1, 311/3750, d=-24.548, g=70.838\n",
      ">1, 312/3750, d=-22.638, g=82.434\n",
      ">1, 313/3750, d=-23.596, g=82.245\n",
      ">1, 314/3750, d=-22.685, g=81.059\n",
      ">1, 315/3750, d=-22.959, g=84.621\n",
      ">1, 316/3750, d=-24.176, g=78.733\n",
      ">1, 317/3750, d=-21.069, g=88.350\n",
      ">1, 318/3750, d=-24.021, g=76.594\n",
      ">1, 319/3750, d=-23.427, g=72.760\n",
      ">1, 320/3750, d=-25.415, g=80.104\n",
      ">1, 321/3750, d=-20.958, g=75.566\n",
      ">1, 322/3750, d=-20.274, g=74.343\n",
      ">1, 323/3750, d=-27.925, g=80.315\n",
      ">1, 324/3750, d=-23.740, g=80.589\n",
      ">1, 325/3750, d=-22.681, g=75.870\n",
      ">1, 326/3750, d=-23.905, g=80.921\n",
      ">1, 327/3750, d=-18.082, g=73.807\n",
      ">1, 328/3750, d=-21.972, g=86.708\n",
      ">1, 329/3750, d=-22.373, g=75.834\n",
      ">1, 330/3750, d=-22.644, g=78.354\n",
      ">1, 331/3750, d=-25.570, g=84.120\n",
      ">1, 332/3750, d=-25.270, g=75.700\n",
      ">1, 333/3750, d=-28.398, g=74.010\n",
      ">1, 334/3750, d=-19.681, g=69.844\n",
      ">1, 335/3750, d=-20.609, g=68.715\n",
      ">1, 336/3750, d=-23.913, g=70.689\n",
      ">1, 337/3750, d=-25.740, g=67.858\n",
      ">1, 338/3750, d=-21.652, g=66.950\n",
      ">1, 339/3750, d=-22.198, g=70.627\n",
      ">1, 340/3750, d=-21.270, g=73.665\n",
      ">1, 341/3750, d=-25.123, g=65.217\n",
      ">1, 342/3750, d=-23.701, g=71.871\n",
      ">1, 343/3750, d=-19.395, g=70.645\n",
      ">1, 344/3750, d=-23.683, g=70.809\n",
      ">1, 345/3750, d=-16.098, g=57.625\n",
      ">1, 346/3750, d=-17.698, g=55.191\n",
      ">1, 347/3750, d=-17.937, g=53.129\n",
      ">1, 348/3750, d=-26.284, g=53.760\n",
      ">1, 349/3750, d=-24.132, g=52.993\n",
      ">1, 350/3750, d=-22.351, g=63.387\n",
      ">1, 351/3750, d=-18.355, g=57.727\n",
      ">1, 352/3750, d=-25.702, g=56.939\n",
      ">1, 353/3750, d=-16.077, g=59.763\n",
      ">1, 354/3750, d=-24.481, g=57.273\n",
      ">1, 355/3750, d=-17.128, g=56.961\n",
      ">1, 356/3750, d=-28.038, g=52.867\n",
      ">1, 357/3750, d=-18.867, g=55.937\n",
      ">1, 358/3750, d=-22.239, g=54.069\n",
      ">1, 359/3750, d=-19.493, g=59.697\n",
      ">1, 360/3750, d=-19.908, g=56.282\n",
      ">1, 361/3750, d=-18.514, g=54.082\n",
      ">1, 362/3750, d=-27.212, g=42.489\n",
      ">1, 363/3750, d=-17.552, g=51.963\n",
      ">1, 364/3750, d=-25.937, g=44.380\n",
      ">1, 365/3750, d=-18.118, g=43.213\n",
      ">1, 366/3750, d=-21.249, g=52.079\n",
      ">1, 367/3750, d=-21.003, g=52.182\n",
      ">1, 368/3750, d=-18.067, g=49.217\n",
      ">1, 369/3750, d=-23.371, g=50.197\n",
      ">1, 370/3750, d=-22.178, g=51.505\n",
      ">1, 371/3750, d=-17.752, g=51.784\n",
      ">1, 372/3750, d=-19.294, g=52.646\n",
      ">1, 373/3750, d=-16.736, g=53.026\n",
      ">1, 374/3750, d=-14.403, g=44.356\n",
      ">1, 375/3750, d=-25.501, g=36.877\n",
      ">1, 376/3750, d=-20.532, g=45.885\n",
      ">1, 377/3750, d=-15.269, g=42.317\n",
      ">1, 378/3750, d=-23.200, g=42.142\n",
      ">1, 379/3750, d=-16.981, g=43.321\n",
      ">1, 380/3750, d=-17.810, g=37.116\n",
      ">1, 381/3750, d=-21.349, g=34.694\n",
      ">1, 382/3750, d=-21.591, g=35.033\n",
      ">1, 383/3750, d=-22.445, g=40.320\n",
      ">1, 384/3750, d=-23.153, g=44.220\n",
      ">1, 385/3750, d=-16.394, g=38.065\n",
      ">1, 386/3750, d=-25.053, g=41.864\n",
      ">1, 387/3750, d=-23.853, g=34.326\n",
      ">1, 388/3750, d=-15.916, g=34.657\n",
      ">1, 389/3750, d=-17.646, g=39.146\n",
      ">1, 390/3750, d=-20.507, g=36.552\n",
      ">1, 391/3750, d=-20.321, g=35.150\n",
      ">1, 392/3750, d=-19.505, g=34.433\n",
      ">1, 393/3750, d=-15.668, g=36.067\n",
      ">1, 394/3750, d=-20.160, g=38.949\n",
      ">1, 395/3750, d=-17.711, g=37.916\n",
      ">1, 396/3750, d=-14.540, g=36.724\n",
      ">1, 397/3750, d=-21.655, g=33.727\n",
      ">1, 398/3750, d=-20.404, g=32.425\n",
      ">1, 399/3750, d=-15.991, g=34.193\n",
      ">1, 400/3750, d=-21.717, g=33.631\n",
      ">1, 401/3750, d=-19.026, g=30.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_400_ep_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_400_ep_0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 402/3750, d=-20.117, g=35.532\n",
      ">1, 403/3750, d=-14.806, g=25.761\n",
      ">1, 404/3750, d=-15.056, g=24.303\n",
      ">1, 405/3750, d=-13.336, g=26.932\n",
      ">1, 406/3750, d=-14.387, g=34.405\n",
      ">1, 407/3750, d=-19.933, g=34.564\n",
      ">1, 408/3750, d=-18.079, g=34.869\n",
      ">1, 409/3750, d=-20.928, g=39.329\n",
      ">1, 410/3750, d=-21.750, g=40.083\n",
      ">1, 411/3750, d=-16.760, g=36.723\n",
      ">1, 412/3750, d=-20.016, g=43.002\n",
      ">1, 413/3750, d=-11.342, g=42.203\n",
      ">1, 414/3750, d=-17.714, g=40.813\n",
      ">1, 415/3750, d=-15.982, g=45.361\n",
      ">1, 416/3750, d=-14.586, g=44.262\n",
      ">1, 417/3750, d=-18.180, g=45.482\n",
      ">1, 418/3750, d=-17.885, g=46.368\n",
      ">1, 419/3750, d=-18.684, g=41.996\n",
      ">1, 420/3750, d=-18.450, g=42.976\n",
      ">1, 421/3750, d=-13.261, g=40.733\n",
      ">1, 422/3750, d=-16.197, g=37.901\n",
      ">1, 423/3750, d=-17.883, g=37.697\n",
      ">1, 424/3750, d=-18.643, g=41.524\n",
      ">1, 425/3750, d=-14.302, g=44.851\n",
      ">1, 426/3750, d=-17.216, g=47.267\n",
      ">1, 427/3750, d=-14.227, g=42.531\n",
      ">1, 428/3750, d=-11.398, g=40.246\n",
      ">1, 429/3750, d=-15.631, g=44.151\n",
      ">1, 430/3750, d=-14.972, g=40.053\n",
      ">1, 431/3750, d=-11.274, g=43.225\n",
      ">1, 432/3750, d=-17.049, g=40.275\n",
      ">1, 433/3750, d=-17.407, g=37.601\n",
      ">1, 434/3750, d=-15.360, g=41.728\n",
      ">1, 435/3750, d=-11.748, g=41.167\n",
      ">1, 436/3750, d=-15.675, g=46.134\n",
      ">1, 437/3750, d=-14.522, g=48.399\n",
      ">1, 438/3750, d=-12.900, g=46.506\n",
      ">1, 439/3750, d=-18.550, g=46.848\n",
      ">1, 440/3750, d=-16.463, g=47.830\n",
      ">1, 441/3750, d=-14.952, g=41.074\n",
      ">1, 442/3750, d=-17.484, g=42.185\n",
      ">1, 443/3750, d=-15.051, g=46.243\n",
      ">1, 444/3750, d=-18.004, g=47.404\n",
      ">1, 445/3750, d=-17.777, g=41.304\n",
      ">1, 446/3750, d=-19.515, g=44.788\n",
      ">1, 447/3750, d=-22.159, g=47.566\n",
      ">1, 448/3750, d=-16.545, g=49.599\n",
      ">1, 449/3750, d=-14.012, g=44.711\n",
      ">1, 450/3750, d=-17.510, g=41.375\n",
      ">1, 451/3750, d=-18.036, g=44.840\n",
      ">1, 452/3750, d=-14.121, g=39.438\n",
      ">1, 453/3750, d=-16.608, g=42.150\n",
      ">1, 454/3750, d=-18.033, g=39.474\n",
      ">1, 455/3750, d=-15.953, g=36.317\n",
      ">1, 456/3750, d=-14.284, g=35.387\n",
      ">1, 457/3750, d=-15.385, g=37.397\n",
      ">1, 458/3750, d=-10.498, g=40.561\n",
      ">1, 459/3750, d=-14.463, g=41.613\n",
      ">1, 460/3750, d=-16.127, g=38.304\n",
      ">1, 461/3750, d=-14.746, g=43.277\n",
      ">1, 462/3750, d=-13.760, g=45.351\n",
      ">1, 463/3750, d=-14.937, g=44.751\n",
      ">1, 464/3750, d=-15.728, g=39.681\n",
      ">1, 465/3750, d=-15.021, g=36.286\n",
      ">1, 466/3750, d=-9.987, g=37.585\n",
      ">1, 467/3750, d=-16.097, g=37.225\n",
      ">1, 468/3750, d=-11.545, g=40.742\n",
      ">1, 469/3750, d=-13.784, g=40.251\n",
      ">1, 470/3750, d=-16.665, g=45.489\n",
      ">1, 471/3750, d=-12.178, g=38.957\n",
      ">1, 472/3750, d=-12.711, g=39.970\n",
      ">1, 473/3750, d=-16.214, g=39.473\n",
      ">1, 474/3750, d=-14.986, g=35.039\n",
      ">1, 475/3750, d=-10.336, g=37.398\n",
      ">1, 476/3750, d=-17.334, g=36.626\n",
      ">1, 477/3750, d=-16.082, g=37.311\n",
      ">1, 478/3750, d=-14.659, g=37.721\n",
      ">1, 479/3750, d=-12.374, g=38.727\n",
      ">1, 480/3750, d=-11.921, g=39.315\n",
      ">1, 481/3750, d=-16.363, g=47.407\n",
      ">1, 482/3750, d=-12.864, g=38.027\n",
      ">1, 483/3750, d=-18.275, g=32.394\n",
      ">1, 484/3750, d=-18.909, g=33.918\n",
      ">1, 485/3750, d=-7.448, g=36.577\n",
      ">1, 486/3750, d=-12.967, g=39.502\n",
      ">1, 487/3750, d=-25.447, g=39.704\n",
      ">1, 488/3750, d=-16.820, g=31.391\n",
      ">1, 489/3750, d=-19.806, g=34.998\n",
      ">1, 490/3750, d=-11.246, g=29.748\n",
      ">1, 491/3750, d=-16.622, g=37.164\n",
      ">1, 492/3750, d=-15.558, g=41.157\n",
      ">1, 493/3750, d=-15.399, g=37.136\n",
      ">1, 494/3750, d=-14.775, g=32.995\n",
      ">1, 495/3750, d=-18.327, g=34.095\n",
      ">1, 496/3750, d=-17.759, g=33.549\n",
      ">1, 497/3750, d=-18.317, g=34.694\n",
      ">1, 498/3750, d=-18.656, g=35.588\n",
      ">1, 499/3750, d=-13.268, g=36.772\n",
      ">1, 500/3750, d=-14.961, g=40.196\n",
      ">1, 501/3750, d=-14.839, g=32.890\n",
      ">1, 502/3750, d=-13.178, g=30.910\n",
      ">1, 503/3750, d=-18.209, g=29.666\n",
      ">1, 504/3750, d=-16.625, g=32.509\n",
      ">1, 505/3750, d=-12.211, g=33.501\n",
      ">1, 506/3750, d=-15.198, g=33.923\n",
      ">1, 507/3750, d=-17.065, g=30.860\n",
      ">1, 508/3750, d=-13.928, g=33.709\n",
      ">1, 509/3750, d=-14.132, g=31.897\n",
      ">1, 510/3750, d=-18.402, g=32.747\n",
      ">1, 511/3750, d=-15.821, g=29.693\n",
      ">1, 512/3750, d=-18.268, g=29.749\n",
      ">1, 513/3750, d=-13.882, g=30.522\n",
      ">1, 514/3750, d=-13.312, g=41.838\n",
      ">1, 515/3750, d=-14.204, g=41.993\n",
      ">1, 516/3750, d=-15.176, g=38.443\n",
      ">1, 517/3750, d=-15.880, g=37.054\n",
      ">1, 518/3750, d=-16.340, g=39.829\n",
      ">1, 519/3750, d=-14.292, g=38.365\n",
      ">1, 520/3750, d=-14.941, g=38.359\n",
      ">1, 521/3750, d=-17.896, g=43.681\n",
      ">1, 522/3750, d=-13.597, g=35.176\n",
      ">1, 523/3750, d=-18.980, g=37.331\n",
      ">1, 524/3750, d=-14.910, g=32.963\n",
      ">1, 525/3750, d=-12.350, g=36.518\n",
      ">1, 526/3750, d=-14.803, g=41.136\n",
      ">1, 527/3750, d=-15.527, g=38.589\n",
      ">1, 528/3750, d=-16.575, g=38.388\n",
      ">1, 529/3750, d=-10.578, g=32.092\n",
      ">1, 530/3750, d=-16.040, g=26.725\n",
      ">1, 531/3750, d=-15.239, g=28.197\n",
      ">1, 532/3750, d=-14.053, g=30.967\n",
      ">1, 533/3750, d=-21.045, g=31.125\n",
      ">1, 534/3750, d=-20.617, g=25.942\n",
      ">1, 535/3750, d=-19.577, g=23.595\n",
      ">1, 536/3750, d=-14.072, g=25.926\n",
      ">1, 537/3750, d=-12.333, g=23.379\n",
      ">1, 538/3750, d=-15.738, g=28.050\n",
      ">1, 539/3750, d=-18.028, g=30.152\n",
      ">1, 540/3750, d=-15.181, g=33.070\n",
      ">1, 541/3750, d=-12.139, g=42.018\n",
      ">1, 542/3750, d=-14.561, g=39.485\n",
      ">1, 543/3750, d=-18.193, g=42.144\n",
      ">1, 544/3750, d=-18.442, g=39.627\n",
      ">1, 545/3750, d=-15.631, g=39.693\n",
      ">1, 546/3750, d=-17.205, g=34.473\n",
      ">1, 547/3750, d=-18.941, g=38.557\n",
      ">1, 548/3750, d=-12.482, g=34.243\n",
      ">1, 549/3750, d=-17.446, g=32.928\n",
      ">1, 550/3750, d=-15.582, g=29.442\n",
      ">1, 551/3750, d=-20.702, g=27.006\n",
      ">1, 552/3750, d=-20.800, g=32.774\n",
      ">1, 553/3750, d=-12.892, g=37.832\n",
      ">1, 554/3750, d=-12.108, g=34.035\n",
      ">1, 555/3750, d=-13.027, g=33.230\n",
      ">1, 556/3750, d=-20.149, g=30.959\n",
      ">1, 557/3750, d=-17.559, g=33.301\n",
      ">1, 558/3750, d=-11.204, g=31.367\n",
      ">1, 559/3750, d=-12.966, g=32.741\n",
      ">1, 560/3750, d=-14.877, g=31.984\n",
      ">1, 561/3750, d=-12.339, g=30.232\n",
      ">1, 562/3750, d=-14.859, g=27.435\n",
      ">1, 563/3750, d=-17.228, g=28.065\n",
      ">1, 564/3750, d=-16.757, g=23.839\n",
      ">1, 565/3750, d=-8.100, g=28.465\n",
      ">1, 566/3750, d=-12.764, g=32.104\n",
      ">1, 567/3750, d=-12.768, g=29.475\n",
      ">1, 568/3750, d=-14.445, g=26.453\n",
      ">1, 569/3750, d=-13.854, g=27.028\n",
      ">1, 570/3750, d=-17.309, g=26.218\n",
      ">1, 571/3750, d=-15.828, g=28.440\n",
      ">1, 572/3750, d=-16.861, g=32.494\n",
      ">1, 573/3750, d=-15.207, g=33.840\n",
      ">1, 574/3750, d=-12.649, g=41.147\n",
      ">1, 575/3750, d=-14.039, g=37.841\n",
      ">1, 576/3750, d=-15.334, g=34.626\n",
      ">1, 577/3750, d=-11.553, g=35.565\n",
      ">1, 578/3750, d=-11.766, g=28.181\n",
      ">1, 579/3750, d=-15.586, g=27.681\n",
      ">1, 580/3750, d=-14.438, g=27.144\n",
      ">1, 581/3750, d=-13.806, g=28.168\n",
      ">1, 582/3750, d=-14.171, g=31.044\n",
      ">1, 583/3750, d=-16.720, g=27.128\n",
      ">1, 584/3750, d=-9.819, g=27.371\n",
      ">1, 585/3750, d=-12.753, g=25.712\n",
      ">1, 586/3750, d=-16.095, g=27.670\n",
      ">1, 587/3750, d=-12.158, g=29.781\n",
      ">1, 588/3750, d=-11.679, g=35.510\n",
      ">1, 589/3750, d=-11.311, g=29.201\n",
      ">1, 590/3750, d=-15.412, g=22.864\n",
      ">1, 591/3750, d=-15.868, g=21.345\n",
      ">1, 592/3750, d=-17.094, g=22.588\n",
      ">1, 593/3750, d=-13.576, g=18.881\n",
      ">1, 594/3750, d=-15.799, g=18.255\n",
      ">1, 595/3750, d=-15.821, g=26.050\n",
      ">1, 596/3750, d=-15.603, g=26.041\n",
      ">1, 597/3750, d=-16.660, g=26.801\n",
      ">1, 598/3750, d=-10.827, g=29.401\n",
      ">1, 599/3750, d=-9.691, g=27.747\n",
      ">1, 600/3750, d=-13.979, g=30.616\n",
      ">1, 601/3750, d=-14.611, g=31.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_600_ep_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_600_ep_0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 602/3750, d=-7.580, g=26.165\n",
      ">1, 603/3750, d=-12.684, g=29.446\n",
      ">1, 604/3750, d=-12.597, g=27.055\n",
      ">1, 605/3750, d=-13.330, g=26.971\n",
      ">1, 606/3750, d=-15.794, g=24.439\n",
      ">1, 607/3750, d=-14.165, g=26.441\n",
      ">1, 608/3750, d=-7.135, g=29.431\n",
      ">1, 609/3750, d=-13.190, g=30.240\n",
      ">1, 610/3750, d=-13.489, g=28.021\n",
      ">1, 611/3750, d=-14.917, g=24.403\n",
      ">1, 612/3750, d=-11.104, g=27.925\n",
      ">1, 613/3750, d=-17.140, g=28.240\n",
      ">1, 614/3750, d=-13.941, g=27.490\n",
      ">1, 615/3750, d=-19.752, g=28.879\n",
      ">1, 616/3750, d=-14.894, g=28.835\n",
      ">1, 617/3750, d=-15.633, g=27.983\n",
      ">1, 618/3750, d=-12.013, g=26.856\n",
      ">1, 619/3750, d=-11.717, g=29.784\n",
      ">1, 620/3750, d=-16.977, g=35.196\n",
      ">1, 621/3750, d=-16.472, g=30.556\n",
      ">1, 622/3750, d=-13.858, g=31.810\n",
      ">1, 623/3750, d=-12.056, g=33.752\n",
      ">1, 624/3750, d=-11.172, g=40.836\n",
      ">1, 625/3750, d=-15.073, g=33.741\n",
      ">1, 626/3750, d=-17.329, g=33.368\n",
      ">1, 627/3750, d=-20.968, g=32.969\n",
      ">1, 628/3750, d=-13.425, g=35.938\n",
      ">1, 629/3750, d=-12.028, g=31.159\n",
      ">1, 630/3750, d=-19.550, g=36.320\n",
      ">1, 631/3750, d=-15.035, g=37.848\n",
      ">1, 632/3750, d=-12.303, g=26.258\n",
      ">1, 633/3750, d=-13.119, g=28.370\n",
      ">1, 634/3750, d=-13.147, g=34.786\n",
      ">1, 635/3750, d=-14.922, g=29.518\n",
      ">1, 636/3750, d=-8.558, g=26.166\n",
      ">1, 637/3750, d=-18.200, g=25.291\n",
      ">1, 638/3750, d=-12.194, g=28.187\n",
      ">1, 639/3750, d=-10.601, g=29.241\n",
      ">1, 640/3750, d=-10.162, g=25.248\n",
      ">1, 641/3750, d=-11.946, g=28.377\n",
      ">1, 642/3750, d=-12.859, g=34.051\n",
      ">1, 643/3750, d=-13.323, g=28.540\n",
      ">1, 644/3750, d=-18.016, g=34.714\n",
      ">1, 645/3750, d=-16.888, g=37.899\n",
      ">1, 646/3750, d=-14.659, g=43.184\n",
      ">1, 647/3750, d=-12.091, g=43.299\n",
      ">1, 648/3750, d=-13.073, g=48.844\n",
      ">1, 649/3750, d=-6.783, g=46.782\n",
      ">1, 650/3750, d=-16.470, g=47.183\n",
      ">1, 651/3750, d=-11.036, g=41.456\n",
      ">1, 652/3750, d=-11.469, g=36.803\n",
      ">1, 653/3750, d=-12.253, g=35.849\n",
      ">1, 654/3750, d=-15.099, g=42.087\n",
      ">1, 655/3750, d=-15.940, g=35.993\n",
      ">1, 656/3750, d=-9.692, g=33.637\n",
      ">1, 657/3750, d=-13.424, g=34.374\n",
      ">1, 658/3750, d=-13.314, g=28.515\n",
      ">1, 659/3750, d=-15.801, g=30.450\n",
      ">1, 660/3750, d=-14.561, g=36.573\n",
      ">1, 661/3750, d=-13.881, g=32.191\n",
      ">1, 662/3750, d=-11.504, g=40.923\n",
      ">1, 663/3750, d=-13.629, g=37.509\n",
      ">1, 664/3750, d=-13.149, g=38.939\n",
      ">1, 665/3750, d=-9.016, g=37.348\n",
      ">1, 666/3750, d=-9.372, g=38.308\n",
      ">1, 667/3750, d=-11.534, g=26.275\n",
      ">1, 668/3750, d=-17.927, g=29.504\n",
      ">1, 669/3750, d=-12.616, g=29.471\n",
      ">1, 670/3750, d=-15.731, g=22.420\n",
      ">1, 671/3750, d=-10.912, g=19.281\n",
      ">1, 672/3750, d=-10.515, g=21.054\n",
      ">1, 673/3750, d=-9.092, g=21.406\n",
      ">1, 674/3750, d=-16.113, g=23.687\n",
      ">1, 675/3750, d=-9.410, g=32.999\n",
      ">1, 676/3750, d=-12.423, g=25.082\n",
      ">1, 677/3750, d=-16.315, g=25.041\n",
      ">1, 678/3750, d=-14.919, g=18.302\n",
      ">1, 679/3750, d=-14.939, g=19.888\n",
      ">1, 680/3750, d=-18.815, g=26.011\n",
      ">1, 681/3750, d=-12.602, g=25.790\n",
      ">1, 682/3750, d=-17.619, g=23.303\n",
      ">1, 683/3750, d=-11.335, g=21.616\n",
      ">1, 684/3750, d=-14.983, g=23.383\n",
      ">1, 685/3750, d=-14.289, g=23.220\n",
      ">1, 686/3750, d=-13.014, g=27.373\n",
      ">1, 687/3750, d=-17.821, g=24.651\n",
      ">1, 688/3750, d=-8.137, g=23.915\n",
      ">1, 689/3750, d=-16.966, g=17.043\n",
      ">1, 690/3750, d=-16.888, g=22.588\n",
      ">1, 691/3750, d=-7.831, g=20.119\n",
      ">1, 692/3750, d=-13.507, g=15.881\n",
      ">1, 693/3750, d=-10.265, g=16.864\n",
      ">1, 694/3750, d=-16.276, g=21.465\n",
      ">1, 695/3750, d=-12.108, g=29.538\n",
      ">1, 696/3750, d=-12.601, g=28.841\n",
      ">1, 697/3750, d=-14.841, g=20.057\n",
      ">1, 698/3750, d=-14.924, g=22.082\n",
      ">1, 699/3750, d=-14.330, g=21.018\n",
      ">1, 700/3750, d=-11.774, g=26.567\n",
      ">1, 701/3750, d=-7.966, g=21.459\n",
      ">1, 702/3750, d=-13.614, g=19.674\n",
      ">1, 703/3750, d=-16.330, g=19.967\n",
      ">1, 704/3750, d=-17.974, g=20.555\n",
      ">1, 705/3750, d=-12.363, g=19.057\n",
      ">1, 706/3750, d=-13.092, g=19.118\n",
      ">1, 707/3750, d=-13.603, g=10.703\n",
      ">1, 708/3750, d=-11.613, g=9.577\n",
      ">1, 709/3750, d=-16.738, g=9.093\n",
      ">1, 710/3750, d=-8.641, g=11.683\n",
      ">1, 711/3750, d=-10.537, g=15.360\n",
      ">1, 712/3750, d=-9.264, g=20.518\n",
      ">1, 713/3750, d=-9.801, g=16.477\n",
      ">1, 714/3750, d=-16.101, g=19.383\n",
      ">1, 715/3750, d=-17.134, g=20.647\n",
      ">1, 716/3750, d=-7.729, g=23.421\n",
      ">1, 717/3750, d=-17.945, g=21.509\n",
      ">1, 718/3750, d=-4.566, g=16.132\n",
      ">1, 719/3750, d=-12.611, g=11.578\n",
      ">1, 720/3750, d=-14.270, g=17.307\n",
      ">1, 721/3750, d=-11.634, g=11.953\n",
      ">1, 722/3750, d=-9.212, g=16.840\n",
      ">1, 723/3750, d=-15.094, g=13.521\n",
      ">1, 724/3750, d=-12.543, g=7.409\n",
      ">1, 725/3750, d=-11.749, g=6.699\n",
      ">1, 726/3750, d=-15.009, g=16.214\n",
      ">1, 727/3750, d=-8.691, g=13.063\n",
      ">1, 728/3750, d=-9.049, g=9.564\n",
      ">1, 729/3750, d=-12.424, g=11.708\n",
      ">1, 730/3750, d=-12.399, g=11.968\n",
      ">1, 731/3750, d=-10.345, g=8.338\n",
      ">1, 732/3750, d=-8.880, g=5.148\n",
      ">1, 733/3750, d=-13.064, g=1.201\n",
      ">1, 734/3750, d=-11.586, g=7.572\n",
      ">1, 735/3750, d=-14.372, g=5.467\n",
      ">1, 736/3750, d=-10.348, g=5.024\n",
      ">1, 737/3750, d=-13.203, g=5.671\n",
      ">1, 738/3750, d=-9.287, g=12.863\n",
      ">1, 739/3750, d=-13.675, g=6.807\n",
      ">1, 740/3750, d=-17.885, g=1.454\n",
      ">1, 741/3750, d=-11.266, g=10.143\n",
      ">1, 742/3750, d=-12.994, g=1.954\n",
      ">1, 743/3750, d=-8.663, g=-1.477\n",
      ">1, 744/3750, d=-10.506, g=-0.727\n",
      ">1, 745/3750, d=-5.791, g=0.855\n",
      ">1, 746/3750, d=-10.843, g=-4.229\n",
      ">1, 747/3750, d=-10.269, g=1.354\n",
      ">1, 748/3750, d=-9.609, g=3.903\n",
      ">1, 749/3750, d=-11.604, g=13.260\n",
      ">1, 750/3750, d=-14.181, g=10.898\n",
      ">1, 751/3750, d=-11.946, g=5.714\n",
      ">1, 752/3750, d=-18.558, g=16.255\n",
      ">1, 753/3750, d=-14.334, g=7.632\n",
      ">1, 754/3750, d=-11.315, g=9.693\n",
      ">1, 755/3750, d=-9.237, g=8.946\n",
      ">1, 756/3750, d=-10.237, g=6.271\n",
      ">1, 757/3750, d=-16.712, g=10.673\n",
      ">1, 758/3750, d=-11.342, g=11.468\n",
      ">1, 759/3750, d=-14.682, g=23.090\n",
      ">1, 760/3750, d=-12.038, g=22.073\n",
      ">1, 761/3750, d=-10.047, g=20.871\n",
      ">1, 762/3750, d=-14.984, g=23.037\n",
      ">1, 763/3750, d=-9.120, g=21.747\n",
      ">1, 764/3750, d=-7.160, g=19.283\n",
      ">1, 765/3750, d=-13.370, g=18.351\n",
      ">1, 766/3750, d=-12.265, g=16.423\n",
      ">1, 767/3750, d=-16.668, g=18.134\n",
      ">1, 768/3750, d=-17.528, g=16.062\n",
      ">1, 769/3750, d=-10.013, g=22.537\n",
      ">1, 770/3750, d=-21.716, g=14.927\n",
      ">1, 771/3750, d=-17.594, g=16.473\n",
      ">1, 772/3750, d=-10.898, g=20.507\n",
      ">1, 773/3750, d=-14.609, g=12.651\n",
      ">1, 774/3750, d=-17.671, g=24.270\n",
      ">1, 775/3750, d=-17.225, g=27.455\n",
      ">1, 776/3750, d=-15.070, g=36.626\n",
      ">1, 777/3750, d=-7.938, g=25.886\n",
      ">1, 778/3750, d=-13.898, g=27.222\n",
      ">1, 779/3750, d=-9.002, g=22.706\n",
      ">1, 780/3750, d=-14.736, g=17.557\n",
      ">1, 781/3750, d=-10.030, g=24.869\n",
      ">1, 782/3750, d=-10.680, g=29.624\n",
      ">1, 783/3750, d=-12.854, g=26.864\n",
      ">1, 784/3750, d=-10.962, g=25.142\n",
      ">1, 785/3750, d=-11.825, g=18.047\n",
      ">1, 786/3750, d=-11.721, g=13.964\n",
      ">1, 787/3750, d=-17.176, g=16.967\n",
      ">1, 788/3750, d=-14.745, g=17.517\n",
      ">1, 789/3750, d=-20.329, g=19.813\n",
      ">1, 790/3750, d=-10.261, g=22.963\n",
      ">1, 791/3750, d=-8.664, g=29.194\n",
      ">1, 792/3750, d=-9.716, g=25.562\n",
      ">1, 793/3750, d=-10.511, g=24.633\n",
      ">1, 794/3750, d=-9.840, g=20.297\n",
      ">1, 795/3750, d=-9.324, g=15.763\n",
      ">1, 796/3750, d=-12.342, g=18.080\n",
      ">1, 797/3750, d=-16.124, g=21.731\n",
      ">1, 798/3750, d=-8.202, g=11.758\n",
      ">1, 799/3750, d=-12.876, g=14.165\n",
      ">1, 800/3750, d=-12.721, g=6.252\n",
      ">1, 801/3750, d=-9.782, g=11.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_800_ep_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exp_result_new_ideas/ep_0/models/i_800_ep_0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 802/3750, d=-10.234, g=10.092\n",
      ">1, 803/3750, d=-7.771, g=8.539\n",
      ">1, 804/3750, d=-14.287, g=8.242\n",
      ">1, 805/3750, d=-8.888, g=-0.713\n",
      ">1, 806/3750, d=-15.629, g=5.540\n",
      ">1, 807/3750, d=-12.497, g=1.668\n",
      ">1, 808/3750, d=-10.068, g=3.396\n",
      ">1, 809/3750, d=-7.633, g=0.064\n",
      ">1, 810/3750, d=-10.253, g=-5.111\n",
      ">1, 811/3750, d=-10.545, g=-2.430\n",
      ">1, 812/3750, d=-16.692, g=1.345\n",
      ">1, 813/3750, d=-17.611, g=5.110\n",
      ">1, 814/3750, d=-11.602, g=-4.936\n",
      ">1, 815/3750, d=-9.185, g=-0.626\n",
      ">1, 816/3750, d=-8.947, g=2.358\n",
      ">1, 817/3750, d=-11.902, g=4.934\n",
      ">1, 818/3750, d=-14.608, g=4.951\n",
      ">1, 819/3750, d=-10.466, g=17.947\n",
      ">1, 820/3750, d=-11.757, g=15.004\n",
      ">1, 821/3750, d=-11.931, g=17.855\n",
      ">1, 822/3750, d=-16.771, g=16.776\n",
      ">1, 823/3750, d=-16.893, g=11.598\n",
      ">1, 824/3750, d=-13.562, g=16.926\n",
      ">1, 825/3750, d=-8.994, g=10.880\n",
      ">1, 826/3750, d=-12.919, g=14.258\n",
      ">1, 827/3750, d=-14.322, g=14.317\n",
      ">1, 828/3750, d=-11.559, g=10.621\n",
      ">1, 829/3750, d=-10.711, g=17.632\n",
      ">1, 830/3750, d=-12.371, g=7.889\n",
      ">1, 831/3750, d=-13.440, g=5.050\n",
      ">1, 832/3750, d=-7.552, g=17.459\n",
      ">1, 833/3750, d=-11.389, g=17.760\n",
      ">1, 834/3750, d=-11.907, g=22.159\n",
      ">1, 835/3750, d=-16.495, g=21.941\n",
      ">1, 836/3750, d=-4.897, g=15.083\n",
      ">1, 837/3750, d=-13.195, g=12.247\n",
      ">1, 838/3750, d=-11.895, g=18.287\n",
      ">1, 839/3750, d=-9.177, g=22.385\n",
      ">1, 840/3750, d=-8.706, g=24.232\n",
      ">1, 841/3750, d=-15.059, g=16.656\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    iteration = train_size // BATCH_SIZE\n",
    "    save_path = os.path.join(SAVE_RESULT, f'ep_{ep}')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, (real_images, real_labels) in enumerate(dataset.take(iteration)):\n",
    "        data_losses = wgan.train_step(real_images=real_images, real_labels=real_labels)\n",
    "        print('>%d, %d/%d, d=%.3f, g=%.3f' %\n",
    "            (ep+1, i+1, iteration, data_losses['d_loss'], data_losses['g_loss']))\n",
    "        if i % 20 == 0:\n",
    "            cbk.on_epoch_end(f'i_{i}_ep_{ep}', save_path=save_path)\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            cbk.save_model(f'{save_path}/models/i_{i}_ep_{ep}')\n",
    "    # Clear session\n",
    "    # Keras iteself has some memory leaks\n",
    "    # Isshue: https://github.com/tensorflow/tensorflow/issues/31312\n",
    "    gcclear_call.on_epoch_end(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a0ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96cc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3bace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "call = g_model.__call__.get_concrete_function(\n",
    "    tf.TensorSpec((1, 1), tf.int32, name='label'), tf.TensorSpec((1, noise_dim), tf.float32, name='noise')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(g_model, f'{SAVE_RESULT}/models/test_save', signatures=call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaff2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(f'{SAVE_RESULT}/models/test_save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26043a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_d = np.random.randn(1, noise_dim).astype(np.float32)\n",
    "random_l = np.random.randint(low=0, high=N_CLASSES, size=(1, 1)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd002e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_np = model(random_l, random_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((output_np[0]+1) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb3feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6e3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd4da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e219a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = g_model.label_layers_l[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w /= np.expand_dims(np.sqrt(np.sum(w * w, axis=1)), axis=1)\n",
    "h = np.dot(w, w.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6385e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
