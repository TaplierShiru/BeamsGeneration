{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/generative/wgan_gp/\n",
    "#\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "\n",
    "# example of loading the generator model and generating images\n",
    "from numpy import asarray\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername2class = {\n",
    "\t'0.0':  0,\n",
    "\t'0.05': 0,\n",
    "\t'0.1':  1,\n",
    "\t'0.15': 1,\n",
    "\t'0.2':  2,\n",
    "\t'0.25': 2,\n",
    "\t'0.3':  3,\n",
    "\t'0.35': 3,\n",
    "\t'0.4':  4,\n",
    "\t'0.45': 4,\n",
    "\t'0.5':  5,\n",
    "\t'0.55': 5,\n",
    "\t'0.6':  6,\n",
    "\t'0.65': 6,\n",
    "\t'0.7':  7,\n",
    "\t'0.75': 7,\n",
    "\t'0.8':  8,\n",
    "\t'0.85': 8,\n",
    "\t'0.9':  9,\n",
    "\t'0.95': 9,\n",
    "\t'1.0':  9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 103.09it/s]\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (336, 336, 3)\n",
    "BATCH_SIZE = 16\n",
    "N_CLASSES = 10\n",
    "# Size of the noise vector\n",
    "noise_dim = 256\n",
    "\n",
    "\n",
    "PATH_DATA = '../../../expand_double_modes'\n",
    "\n",
    "train_images_path = []\n",
    "\n",
    "iterator = tqdm(glob.glob(PATH_DATA + \"/*\"))\n",
    "for single_folder in iterator:\n",
    "    img_folder = shuffle(glob.glob(single_folder + '/*'))\n",
    "    for indx, single_img_path in enumerate(img_folder):\n",
    "        train_images_path.append(single_img_path)\n",
    "iterator.close()\n",
    "\n",
    "train_images_path = shuffle(train_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37800/37800 [01:41<00:00, 371.96it/s]\n"
     ]
    }
   ],
   "source": [
    "CLASS_TO_IMGS = {}\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "  images = (images - 127.5) / 127.5\n",
    "  return images.astype('float32')\n",
    "\n",
    "def load_dataset(path_list: list):\n",
    "    iterator = tqdm(path_list)\n",
    "    for single_path in iterator:\n",
    "        label_s = foldername2class[single_path.split('/')[-2]]\n",
    "        image_s = np.asarray(io.imread(single_path), dtype=np.float32)[..., :3]\n",
    "        if CLASS_TO_IMGS.get(str(label_s)) is None:\n",
    "            CLASS_TO_IMGS[str(label_s)] = []\n",
    "        CLASS_TO_IMGS[str(label_s)] += [image_s]\n",
    "    iterator.close()\n",
    "load_dataset(train_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('/home/rustam/USA/beams/gan_tests/wgan/exp_result_new_ideas/ep_4/models/i_0_ep_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate certain class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GENERATE = 3_600\n",
    "CLASS_GENERATE = 9\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "iterator = tqdm(range(N_GENERATE))\n",
    "for i in iterator:\n",
    "    random_latent_vectors = np.random.normal(size=(1, noise_dim)).astype(np.float32)\n",
    "    gen_img = model(label_i=np.array([[CLASS_GENERATE]], dtype=np.int32), noise_i=random_latent_vectors)[0]\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    gen_img = (gen_img + 1) / 2.0\n",
    "    gen_img = np.clip(gen_img * 255.0, 0, 255).astype(np.float32)\n",
    "    generated_images.append(gen_img)\n",
    "iterator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1 = generated_images\n",
    "images2 = CLASS_TO_IMGS[str(CLASS_GENERATE)][:N_GENERATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:05<00:00, 282.44it/s]\n",
      "100%|██████████| 1500/1500 [00:05<00:00, 274.50it/s]\n",
      "100%|██████████| 1500/1500 [00:05<00:00, 290.18it/s]\n",
      "100%|██████████| 1500/1500 [00:05<00:00, 292.23it/s]\n",
      "100%|██████████| 1500/1500 [00:05<00:00, 275.42it/s]\n",
      "100%|██████████| 1500/1500 [00:05<00:00, 282.36it/s]\n",
      "100%|██████████| 1500/1500 [00:05<00:00, 277.12it/s]\n",
      "100%|██████████| 1500/1500 [00:05<00:00, 287.85it/s]\n",
      "100%|██████████| 1500/1500 [00:05<00:00, 278.16it/s]\n",
      "100%|██████████| 1500/1500 [00:05<00:00, 281.54it/s]\n"
     ]
    }
   ],
   "source": [
    "N_GENERATE = 1_500\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "for class_i in range(N_CLASSES):\n",
    "    iterator = tqdm(range(N_GENERATE))\n",
    "    for _ in iterator:\n",
    "        random_latent_vectors = np.random.normal(size=(1, noise_dim)).astype(np.float32)\n",
    "        gen_img = model(label_i=np.array([[class_i]], dtype=np.int32), noise_i=random_latent_vectors)[0]\n",
    "        # scale from [-1,1] to [0,1]\n",
    "        gen_img = (gen_img + 1) / 2.0\n",
    "        gen_img = np.clip(gen_img * 255.0, 0, 255).astype(np.float32)\n",
    "        generated_images.append(gen_img)\n",
    "    iterator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1 = generated_images\n",
    "images2 = []\n",
    "for class_i in range(N_CLASSES):\n",
    "    images2 += CLASS_TO_IMGS[str(class_i)][:N_GENERATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [05:04<00:00,  5.25s/it]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID (same): 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [05:02<00:00,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID (different): 173.583\n"
     ]
    }
   ],
   "source": [
    "# example of calculating the frechet inception distance in Keras\n",
    "import numpy\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from scipy.linalg import sqrtm\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from skimage.transform import resize\n",
    "\n",
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape):\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        # resize with nearest neighbor interpolation\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        # store\n",
    "        images_list.append(new_image)\n",
    "    return asarray(images_list)\n",
    "\n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images1, images2, batch_calc=256):\n",
    "    assert len(images1) == len(images2)\n",
    "    n_batches = len(images1) // batch_calc\n",
    "    images1, images2 = images1[:n_batches * batch_calc], images2[:n_batches * batch_calc]\n",
    "    iterator = tqdm(range(n_batches))\n",
    "    res_1 = []\n",
    "    res_2 = []\n",
    "    \n",
    "    for i in iterator:\n",
    "        i1_b, i2_b = (\n",
    "            np.array(images1[i * batch_calc: (i+1)*batch_calc], dtype=np.float32), \n",
    "            np.array(images2[i * batch_calc: (i+1)*batch_calc], dtype=np.float32) \n",
    "        )\n",
    "        # resize\n",
    "        i1_b, i2_b = (\n",
    "            scale_images(i1_b, (299,299,3)), \n",
    "            scale_images(i2_b, (299,299,3))\n",
    "        )\n",
    "        # pre-process\n",
    "        i1_b, i2_b = (preprocess_input(i1_b), preprocess_input(i2_b))\n",
    "        # calculate activations\n",
    "        act1 = model.predict(i1_b)\n",
    "        act2 = model.predict(i2_b)\n",
    "        \n",
    "        res_1.append(act1)\n",
    "        res_2.append(act2)\n",
    "    iterator.close()\n",
    "    # calculate activations\n",
    "    act1 = np.concatenate(res_1, axis=0)\n",
    "    act2 = np.concatenate(res_2, axis=0)\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # calculate score\n",
    "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "# prepare the inception v3 model\n",
    "model_incv3 = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
    "# fid between images1 and images1\n",
    "fid = calculate_fid(model_incv3, images1, images1)\n",
    "print('FID (same): %.3f' % fid)\n",
    "# fid between images1 and images2\n",
    "fid = calculate_fid(model_incv3, images1, images2)\n",
    "print('FID (different): %.3f' % fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 8\n",
    "# 3_600 - 255\n",
    "\n",
    "# class 9\n",
    "# 3_600 - \n",
    "\n",
    "# All dataset, \n",
    "# each class 2_000 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of calculating the frechet inception distance in Keras\n",
    "import numpy\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from scipy.linalg import sqrtm\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from skimage.transform import resize\n",
    "\n",
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape):\n",
    "\timages_list = list()\n",
    "\tfor image in images:\n",
    "\t\t# resize with nearest neighbor interpolation\n",
    "\t\tnew_image = resize(image, new_shape, 0)\n",
    "\t\t# store\n",
    "\t\timages_list.append(new_image)\n",
    "\treturn asarray(images_list)\n",
    "\n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images1, images2):\n",
    "\t# calculate activations\n",
    "\tact1 = model.predict(images1)\n",
    "\tact2 = model.predict(images2)\n",
    "\t# calculate mean and covariance statistics\n",
    "\tmu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "\tmu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "\t# calculate sum squared difference between means\n",
    "\tssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
    "\t# calculate sqrt of product between cov\n",
    "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
    "\t# check and correct imaginary numbers from sqrt\n",
    "\tif iscomplexobj(covmean):\n",
    "\t\tcovmean = covmean.real\n",
    "\t# calculate score\n",
    "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\treturn fid\n",
    "\n",
    "# prepare the inception v3 model\n",
    "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
    "# define two fake collections of images\n",
    "images1 = randint(0, 255, 10*32*32*3)\n",
    "images1 = images1.reshape((10,32,32,3))\n",
    "images2 = randint(0, 255, 10*32*32*3)\n",
    "images2 = images2.reshape((10,32,32,3))\n",
    "print('Prepared', images1.shape, images2.shape)\n",
    "# convert integer to floating point values\n",
    "images1 = images1.astype('float32')\n",
    "images2 = images2.astype('float32')\n",
    "# resize images\n",
    "images1 = scale_images(images1, (299,299,3))\n",
    "images2 = scale_images(images2, (299,299,3))\n",
    "print('Scaled', images1.shape, images2.shape)\n",
    "# pre-process images\n",
    "images1 = preprocess_input(images1)\n",
    "images2 = preprocess_input(images2)\n",
    "# fid between images1 and images1\n",
    "fid = calculate_fid(model, images1, images1)\n",
    "print('FID (same): %.3f' % fid)\n",
    "# fid between images1 and images2\n",
    "fid = calculate_fid(model, images1, images2)\n",
    "print('FID (different): %.3f' % fid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
